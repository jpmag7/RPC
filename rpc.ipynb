{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b42dc02c-ab81-4fd0-89bc-782481558a2e","_uuid":"e94b73d6-c4ce-416c-949c-37c3a3df77e7","collapsed":false,"execution":{"iopub.execute_input":"2024-07-29T13:00:06.696236Z","iopub.status.busy":"2024-07-29T13:00:06.695845Z","iopub.status.idle":"2024-07-29T13:00:45.301376Z","shell.execute_reply":"2024-07-29T13:00:45.300119Z","shell.execute_reply.started":"2024-07-29T13:00:06.696197Z"},"jupyter":{"outputs_hidden":false},"scrolled":true,"trusted":true},"outputs":[],"source":["!pip install keras-nlp==0.10.0\n","!pip install faiss-cpu\n","!pip install keras==2.15.0\n","!pip install tensorflow==2.15.0"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"f40f8265-dd13-4005-9b98-0e8497ebd05c","_uuid":"2d66e846-b199-412a-9ca2-1e638ebef526","collapsed":false,"execution":{"iopub.execute_input":"2024-07-29T13:00:45.303843Z","iopub.status.busy":"2024-07-29T13:00:45.303474Z","iopub.status.idle":"2024-07-29T13:01:02.251170Z","shell.execute_reply":"2024-07-29T13:01:02.250216Z","shell.execute_reply.started":"2024-07-29T13:00:45.303809Z"},"jupyter":{"outputs_hidden":false},"scrolled":true,"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from keras.layers import *\n","import keras_nlp\n","\n","import math\n","import spacy\n","import numpy as np\n","import random\n","import json"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5408a093-8dc1-4d25-bc4a-baa07b73f1aa","_uuid":"f2cf7ba5-811c-4dd3-8884-a5ec196c9707","collapsed":false,"execution":{"iopub.execute_input":"2024-07-29T13:01:02.261302Z","iopub.status.busy":"2024-07-29T13:01:02.260890Z","iopub.status.idle":"2024-07-29T13:01:08.793271Z","shell.execute_reply":"2024-07-29T13:01:08.792114Z","shell.execute_reply.started":"2024-07-29T13:01:02.261266Z"},"jupyter":{"outputs_hidden":false},"scrolled":true,"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer\n","from tokenizers import AddedToken\n","\n","tokenizer = AutoTokenizer.from_pretrained('google/t5-v1_1-base')\n","tokenizer.add_tokens(AddedToken(\"\\n\", normalized=False))\n","tokenizer.add_tokens(AddedToken(\"<s>\", normalized=False))\n","vocab_size = len(tokenizer.get_vocab().keys())\n","print(\"vocab_size:\", vocab_size)\n","\n","text = \"<s>Hello hello, how are you today? good, just understanding tokenizers...\\n\"\n","tokens = tokenizer.encode(text, add_special_tokens=False)\n","print(tokens)\n","for t in tokens: print(t, tokenizer.decode([t]))\n","text = tokenizer.decode(tokens, skip_special_tokens=True)\n","print(text)\n","print(tokenizer.pad_token)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"62597560-9627-41e4-a0a2-130a5862e84f","_uuid":"c5b9dc33-f611-4727-9430-74be38173eb6","collapsed":false,"execution":{"iopub.execute_input":"2024-07-29T13:01:08.841517Z","iopub.status.busy":"2024-07-29T13:01:08.841083Z","iopub.status.idle":"2024-07-29T13:04:33.321016Z","shell.execute_reply":"2024-07-29T13:04:33.319775Z","shell.execute_reply.started":"2024-07-29T13:01:08.841481Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["DATASET_PATH = \"dataset.json\"\n","\n","file = open(DATASET_PATH, \"r\")\n","dataset = json.loads(file.read())\n","file.close()\n","\n","data = []\n","for text in dataset:\n","    text = \"\".join(text)\n","    text = tokenizer.encode(\"<s>\" + text, add_special_tokens=False)\n","    data.append(text)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-20T00:17:17.070461Z","iopub.status.busy":"2024-07-20T00:17:17.070057Z","iopub.status.idle":"2024-07-20T00:17:39.446550Z","shell.execute_reply":"2024-07-20T00:17:39.445013Z","shell.execute_reply.started":"2024-07-20T00:17:17.070430Z"},"trusted":true},"outputs":[],"source":["nlp = spacy.load(\"en_core_web_lg\")\n","nlp.max_length = 2000000\n","\n","# 'PART', 'INTJ', 'SPACE', 'AUX', 'PUNCT', 'SYM', 'X', 'SCONJ', 'NUM', 'NOUN', 'ADP', 'ADJ', 'ADV', 'PRON', 'DET', 'CCONJ', 'PROPN', 'VERB'\n","selected = {'NUM', 'NOUN', 'ADJ', 'ADV', 'PROPN'}\n","\n","all_toks = sorted(list(tokenizer.get_vocab().items()), key=lambda x:x[1])\n","all_toks_text = \"\\n\".join([t[0].replace(\"â–\", \"\") for t in all_toks])\n","\n","doc = nlp(all_toks_text)\n","\n","carry_toks = set()\n","\n","print(len(doc), len(all_toks))\n","\n","i = 0\n","for ii, token in enumerate(doc):\n","    if str(token) in all_toks[i][0]: pass\n","    else: i += 1\n","    if str(token) in all_toks[i][0] and token.pos_ in selected and i > 100:\n","        carry_toks.add(all_toks[i][1])\n","print(len(carry_toks))"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d8902183-0b87-4185-910d-46d2bfb7b2f5","_uuid":"29ab352a-86e4-4b60-a5a3-917a77582a86","collapsed":false,"execution":{"iopub.execute_input":"2024-07-20T00:17:39.449477Z","iopub.status.busy":"2024-07-20T00:17:39.448747Z","iopub.status.idle":"2024-07-20T00:17:39.482851Z","shell.execute_reply":"2024-07-20T00:17:39.481421Z","shell.execute_reply.started":"2024-07-20T00:17:39.449442Z"},"jupyter":{"outputs_hidden":false},"scrolled":true,"trusted":true},"outputs":[],"source":["data_size = len(data)\n","\n","def get_random_sample(input_size):\n","    rnd  = random.randint(0, data_size-1)\n","    text = data[rnd]\n","    pos  = random.randint(min(input_size, len(text)-1), len(text)-1)\n","    text = text[:pos]\n","    x    = text[-input_size:]\n","    \n","    in_past = set()\n","    weights = []\n","    for t in x:\n","        if t in carry_toks:\n","            if t in in_past:\n","                weights.append(1.0)\n","            else:\n","                in_past.add(t)\n","                weights.append(0.6)\n","        elif t != tokenizer.pad_token_id:\n","            weights.append(0.6)\n","        else: break\n","    x = x + [tokenizer.pad_token_id] * (input_size - len(x))\n","    weights = weights + [0.0] * (input_size - len(weights))\n","    return x, weights\n","\n","x, w = get_random_sample(256)\n","print(tokenizer.decode(x[:-1]), \"\\n>\", tokenizer.decode(x[1:]))"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"72475573-1f9c-40f0-a3bf-d7c3eafecc1f","_uuid":"d4413b24-015d-42a5-beb0-f4ddbd11f21d","collapsed":false,"execution":{"iopub.execute_input":"2024-07-20T00:17:39.484813Z","iopub.status.busy":"2024-07-20T00:17:39.484405Z","iopub.status.idle":"2024-07-20T00:17:39.499700Z","shell.execute_reply":"2024-07-20T00:17:39.498221Z","shell.execute_reply.started":"2024-07-20T00:17:39.484772Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["def get_train_batch(batch_size, input_size):\n","    X = []\n","    W = []\n","    for _ in range(batch_size):\n","        x, w = get_random_sample(input_size)\n","        X.append(x)\n","        W.append(w)\n","    X = tf.constant(X, shape=(len(X), input_size), dtype=tf.int32)\n","    W = tf.constant(W, shape=(len(W), input_size), dtype=tf.float32)\n","    return X, W"]},{"cell_type":"code","execution_count":6,"metadata":{"_cell_guid":"32ed2cbd-b388-41be-9ba1-55746098c1f7","_uuid":"a087ac59-0689-45c1-b6c9-33d8369c3b65","collapsed":false,"execution":{"iopub.execute_input":"2024-07-29T13:04:33.332856Z","iopub.status.busy":"2024-07-29T13:04:33.332440Z","iopub.status.idle":"2024-07-29T13:04:33.351931Z","shell.execute_reply":"2024-07-29T13:04:33.350693Z","shell.execute_reply.started":"2024-07-29T13:04:33.332825Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["def roll_embeddings(tensor, shift_values):\n","    batch_size, time_size, embed_size = tensor.shape\n","    if batch_size is None: return tensor\n","\n","    shift_matrix = tf.reshape(shift_values, (1, -1, 1))\n","    shift_matrix = tf.tile(shift_matrix, [batch_size, 1, embed_size])\n","    \n","    indices = tf.range(embed_size)\n","    indices_matrix = tf.tile(indices, [batch_size * time_size])\n","    indices_matrix = tf.reshape(indices_matrix, (batch_size, time_size, embed_size))\n","    \n","    new_indices = (indices_matrix + shift_matrix) % embed_size\n","    \n","    rolled_tensor = tf.gather(tensor, new_indices, batch_dims=2)\n","    \n","    return rolled_tensor"]},{"cell_type":"code","execution_count":7,"metadata":{"_cell_guid":"12324a6d-1ec9-4399-954c-cd5be19cb966","_uuid":"980ca45f-b567-46b0-9627-bb79e7d4f3ff","collapsed":false,"execution":{"iopub.execute_input":"2024-07-29T13:04:33.353583Z","iopub.status.busy":"2024-07-29T13:04:33.353238Z","iopub.status.idle":"2024-07-29T13:04:33.372780Z","shell.execute_reply":"2024-07-29T13:04:33.371677Z","shell.execute_reply.started":"2024-07-29T13:04:33.353555Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["class Attention(keras.layers.Layer):\n","    def __init__(self, **kwargs):\n","        super(Attention, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        self.embed_size = input_shape[-1]\n","        self.mask = tf.where(tf.linalg.band_part(tf.ones((input_shape[-2], input_shape[-2])), -1, 0) == 1.0, 0.0, float(\"-inf\"))\n","        self.range_do = -tf.range(input_shape[-2])-1\n","        self.range_undo = tf.range(input_shape[-2])+1\n","        self.Q = self.add_weight(name='kernelQ',\n","                                      shape=(input_shape[-1], input_shape[-1]),\n","                                      initializer='uniform',\n","                                      trainable=True)\n","        self.K = self.add_weight(name='kernelK',\n","                                      shape=(input_shape[-1], input_shape[-1]),\n","                                      initializer='uniform',\n","                                      trainable=True)\n","        self.V = self.add_weight(name='kernelV',\n","                                      shape=(input_shape[-1], input_shape[-1]),\n","                                      initializer='uniform',\n","                                      trainable=True)\n","        super(Attention, self).build(input_shape)\n","\n","    def call(self, x, pos, ret=False):\n","        q    = x @ self.Q\n","        k    = x @ self.K\n","        v    = x @ self.V\n","        atti = tf.matmul(q, k,   transpose_b=True)\n","        attp = tf.matmul(q, pos, transpose_b=True)\n","        attp = roll_embeddings(attp, self.range_do)\n","        att  = atti + attp\n","        att  = tf.nn.softmax((att / math.sqrt(self.embed_size)) + self.mask, axis=-1)\n","        outi = att @ v\n","        attp = roll_embeddings(att, self.range_undo)\n","        outp = attp @ pos\n","        out  = outi + outp\n","        return out"]},{"cell_type":"code","execution_count":8,"metadata":{"_cell_guid":"061315bd-b3f4-41e2-9edf-a2d43e1258ca","_uuid":"40418cf9-3995-4ca2-b619-d8c8f8c37d1e","collapsed":false,"execution":{"iopub.execute_input":"2024-07-29T13:04:33.374699Z","iopub.status.busy":"2024-07-29T13:04:33.374267Z","iopub.status.idle":"2024-07-29T13:04:33.390319Z","shell.execute_reply":"2024-07-29T13:04:33.389099Z","shell.execute_reply.started":"2024-07-29T13:04:33.374660Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["def masked_accuracy(y_true, y_pred, padding_token=tokenizer.pad_token_id):\n","    y_true = tf.cast(y_true, tf.int32)\n","    y_pred = tf.cast(tf.argmax(y_pred, axis=-1), tf.int32)\n","\n","    mask = tf.cast(tf.not_equal(y_true, padding_token), tf.float32)\n","    matches = tf.cast(tf.equal(y_true, y_pred), tf.float32)\n","    \n","    accuracy = tf.reduce_sum(matches * mask) / tf.reduce_sum(mask)\n","    return accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5ecf2d11-ab90-4f14-94fa-a9ec89108e47","_uuid":"e0a95c1e-b00c-4a1f-88d5-c3d08c4cc824","collapsed":false,"execution":{"iopub.execute_input":"2024-07-29T13:04:33.392916Z","iopub.status.busy":"2024-07-29T13:04:33.391851Z","iopub.status.idle":"2024-07-29T13:04:35.803713Z","shell.execute_reply":"2024-07-29T13:04:35.802640Z","shell.execute_reply.started":"2024-07-29T13:04:33.392883Z"},"jupyter":{"outputs_hidden":false},"scrolled":true,"trusted":true},"outputs":[],"source":["input_size = 512\n","embed_size = 128\n","vocab_size = len(tokenizer.get_vocab().keys()) + 1\n","\n","# Encoder\n","inputs_enc = Input(shape=(input_size, ), dtype=tf.int32)\n","emb_layer = Embedding(vocab_size, embed_size)\n","pos_layer = keras_nlp.layers.PositionEmbedding(input_size)\n","\n","x = LayerNormalization()(emb_layer(inputs_enc))\n","pos = pos_layer(x)\n","\n","b = 4\n","for _ in range(b):\n","    x += b**-0.5 * LayerNormalization()(Attention()(x, pos))\n","\n","encoder = keras.Model(inputs=inputs_enc, outputs=x)\n","\n","# Decoder\n","inputs = Input(shape=(input_size, ), dtype=tf.int32)\n","x = encoder(inputs)\n","lm_head = Lambda(lambda x: tf.nn.softmax(tf.matmul(x, emb_layer.embeddings, transpose_b=True), axis=-1))\n","\n","b = 4\n","for _ in range(b):\n","    x1 = Dense(embed_size, activation=\"gelu\")(x)\n","    x1 = Dense(embed_size, activation=\"gelu\")(x1)\n","    x += b**-0.5 * LayerNormalization()(x1)\n","\n","x = lm_head(x)\n","\n","model = keras.Model(inputs=inputs, outputs=x)\n","model.compile(\n","    loss=keras.losses.SparseCategoricalCrossentropy(ignore_class=tokenizer.pad_token_id),\n","    optimizer=keras.optimizers.AdamW(learning_rate=0.001),\n","    metrics=[masked_accuracy, keras_nlp.metrics.Perplexity(mask_token_id=tokenizer.pad_token_id)],\n",")\n","\n","encoder.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"22dadeab-5d9b-4ec2-8a52-8ce784ddbc06","_uuid":"ebdd11a0-e5ea-4948-8a9f-64895de659e4","collapsed":false,"execution":{"iopub.execute_input":"2024-07-20T00:17:39.502455Z","iopub.status.busy":"2024-07-20T00:17:39.502057Z"},"jupyter":{"outputs_hidden":false},"scrolled":true,"trusted":true},"outputs":[],"source":["for i in range(70):\n","    x, w = get_train_batch(4096*8, input_size+1)\n","    if i > 10 and i < 25:\n","        w = tf.where(w < 0.9, 0.05, 1.0)\n","    model.fit(x=x[:, :-1], y=x[:, 1:], shuffle=True, epochs=1, batch_size=16, sample_weight=w[:, 1:])\n","    model.save(\"model_slm.hdf5\")"]},{"cell_type":"code","execution_count":14,"metadata":{"_cell_guid":"62ff191d-4ec6-4fa7-a9b8-dad29e9c120a","_uuid":"b3149792-0bcb-455e-902d-0a2090c602cf","collapsed":false,"execution":{"iopub.execute_input":"2024-07-29T13:04:35.805547Z","iopub.status.busy":"2024-07-29T13:04:35.805183Z","iopub.status.idle":"2024-07-29T13:04:37.638388Z","shell.execute_reply":"2024-07-29T13:04:37.637284Z","shell.execute_reply.started":"2024-07-29T13:04:35.805516Z"},"jupyter":{"outputs_hidden":false},"scrolled":true,"trusted":true},"outputs":[],"source":["model = keras.models.load_model(\n","    \"model_slm.hdf5\",\n","    custom_objects={\n","        \"Attention\"       : Attention,\n","        \"masked_accuracy\" : masked_accuracy,\n","    },\n","    safe_mode=False,\n",")\n","# Extract Encoder\n","encoder = model.layers[1]"]},{"cell_type":"code","execution_count":11,"metadata":{"_cell_guid":"855c5834-248c-4415-95fb-b2d8c2cc7eca","_uuid":"591038fb-c92f-41c3-b849-7a02cb96c171","collapsed":false,"execution":{"iopub.execute_input":"2024-07-29T13:08:30.049561Z","iopub.status.busy":"2024-07-29T13:08:30.049014Z","iopub.status.idle":"2024-07-29T13:08:30.252628Z","shell.execute_reply":"2024-07-29T13:08:30.251460Z","shell.execute_reply.started":"2024-07-29T13:08:30.049515Z"},"jupyter":{"outputs_hidden":false},"scrolled":true,"trusted":true},"outputs":[],"source":["def vectorize_texts(all_texts):\n","    batch_size = 128\n","    vects = []\n","    for i in range(len(all_texts) // batch_size + 1):\n","        texts = all_texts[i*batch_size:i*batch_size+batch_size]\n","        toks = [text + ([tokenizer.pad_token_id] * (input_size - len(text))) for text in texts]\n","        toks = tf.constant(toks, shape=(len(toks), input_size))\n","        vect = encoder(toks)\n","        for v, t in zip(vect, texts):\n","            vects.append(v[:len(t), :])\n","    return tf.concat(vects, axis=0)\n","\n","vectorize_texts([tokenizer.encode(\"Hello. How have you been?\"), tokenizer.encode(\"hello\")])"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9cd9518a-f88a-46d0-88ef-31fd6fc81de6","_uuid":"0b763816-dd9c-4b22-894e-75d0ae5e1cdd","collapsed":false,"execution":{"iopub.execute_input":"2024-07-29T13:08:31.224258Z","iopub.status.busy":"2024-07-29T13:08:31.222829Z","iopub.status.idle":"2024-07-29T14:26:11.184387Z","shell.execute_reply":"2024-07-29T14:26:11.183110Z","shell.execute_reply.started":"2024-07-29T13:08:31.224184Z"},"jupyter":{"outputs_hidden":false},"scrolled":true,"trusted":true},"outputs":[],"source":["all_toks = []\n","prompt_embeds = []\n","\n","batch_size = 128\n","batch = []\n","cur_batch_size = 0\n","\n","for j, text in enumerate(data):\n","    text_size = min(len(text), input_size+1)\n","    all_toks += text[1:text_size]\n","    trail = text[:text_size-1]\n","    \n","    batch.append(trail)\n","    cur_batch_size += 1\n","    \n","    if cur_batch_size >= batch_size:\n","        prompt_embeds.append(vectorize_texts(batch))\n","        cur_batch_size = 0\n","        batch = []\n","        print(j)"]},{"cell_type":"code","execution_count":20,"metadata":{"_cell_guid":"64ddca83-30fa-4c91-a167-f309f2bef98a","_uuid":"9c071e43-a9eb-407d-a55b-4d9ac999e24d","collapsed":false,"execution":{"iopub.execute_input":"2024-07-29T14:26:11.187336Z","iopub.status.busy":"2024-07-29T14:26:11.186848Z","iopub.status.idle":"2024-07-29T14:26:17.035055Z","shell.execute_reply":"2024-07-29T14:26:17.033947Z","shell.execute_reply.started":"2024-07-29T14:26:11.187291Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["prompt_embeds = np.vstack(prompt_embeds).reshape((sum([len(v) for v in prompt_embeds]), embed_size))"]},{"cell_type":"code","execution_count":21,"metadata":{"_cell_guid":"a550bd69-2793-41a9-acf3-33b11130c37c","_uuid":"37021c6f-4309-4844-b750-e47c8368a24b","collapsed":false,"execution":{"iopub.execute_input":"2024-07-29T14:26:17.037085Z","iopub.status.busy":"2024-07-29T14:26:17.036690Z","iopub.status.idle":"2024-07-29T14:26:26.195495Z","shell.execute_reply":"2024-07-29T14:26:26.194217Z","shell.execute_reply.started":"2024-07-29T14:26:17.037053Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["import faiss\n","\n","index = faiss.IndexFlat(embed_size) # IndexHNSWFlat(embed_size, 32)\n","#index.train(prompt_embeds)\n","index.add(prompt_embeds)"]},{"cell_type":"code","execution_count":31,"metadata":{"_cell_guid":"f8ec8b2f-6361-4237-96b0-dc858c6301de","_uuid":"8dca5e69-f59e-46b0-ab4f-d1addfdbcfaa","collapsed":false,"execution":{"iopub.execute_input":"2024-07-29T14:49:07.783141Z","iopub.status.busy":"2024-07-29T14:49:07.782676Z","iopub.status.idle":"2024-07-29T14:51:25.787919Z","shell.execute_reply":"2024-07-29T14:51:25.786816Z","shell.execute_reply.started":"2024-07-29T14:49:07.783092Z"},"jupyter":{"outputs_hidden":false},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<s> Peter: Hello there!\n"," Mia: Hello there, do you follow baseball or MLB much?\n"," Peter: I do, although I haven't followed it much the past couple of years.\n"," Mia: I am the opposite, I have been following more closely these last few, especially this last season as my team almost made the world series.\n"," Peter: Their arrow means they cover everything from the old days.\n"," Mia: Nice, yeah I like it. Did you know that the cubs were the first team to win back to back World Series.\n"," Peter: Women were not allowed to wear a baseball uniform as to be able to play for their teams if the need arises\n"," Mia: Maybe it is his"]}],"source":["text1 = \"\"\"<s>Peter: Hello there!\\n\"\"\"\n","\n","text2 = \"\"\"<s>The dog is red and has five legs.\n","User: What color is the dog?\n","Assistant: red\n","User: How many legs does the dog have?\n","Assistant:\"\"\"\n","\n","k = 10\n","temp = 0.01\n","text = text1\n","size = 128\n","\n","enc_text = tokenizer.encode(text, add_special_tokens=False)\n","text     = tokenizer.decode(enc_text)\n","print(text, end=\"\")\n","\n","for t in range(size):    \n","    xq = vectorize_texts([enc_text])[-1]\n","    xq = np.array(xq).reshape((1, embed_size))\n","    D, I = index.search(xq, k)\n","    toks = [all_toks[i] for i in I[0]]\n","    dists_sft = tf.nn.softmax(-D[0] / temp, axis=-1)\n","    c = tf.random.categorical(tf.math.log([dists_sft]), num_samples=1)[0][0]\n","    tok = toks[c]\n","    \n","    enc_text += [tok]\n","    new_text = tokenizer.decode(enc_text)\n","    \n","    print(new_text[len(text):], end=\"\")\n","    \n","    text = new_text"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4261095,"sourceId":9059404,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":4}
