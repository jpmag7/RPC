{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-01T16:34:06.041102Z",
     "iopub.status.busy": "2025-03-01T16:34:06.040783Z",
     "iopub.status.idle": "2025-03-01T16:34:49.236547Z",
     "shell.execute_reply": "2025-03-01T16:34:49.234814Z",
     "shell.execute_reply.started": "2025-03-01T16:34:06.041075Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-nlp==0.10.0\n",
    "!pip install keras==2.15.0\n",
    "!pip install tensorflow==2.15.0\n",
    "!pip install faiss-cpu==1.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T16:34:49.239070Z",
     "iopub.status.busy": "2025-03-01T16:34:49.238731Z",
     "iopub.status.idle": "2025-03-01T16:35:10.440457Z",
     "shell.execute_reply": "2025-03-01T16:35:10.439415Z",
     "shell.execute_reply.started": "2025-03-01T16:34:49.239040Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-01 16:34:53.205983: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-01 16:34:53.206201: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-01 16:34:53.486260: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import *\n",
    "import keras_nlp\n",
    "from keras import backend\n",
    "\n",
    "import re\n",
    "import requests\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import string\n",
    "import nltk\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T16:35:10.442117Z",
     "iopub.status.busy": "2025-03-01T16:35:10.441571Z",
     "iopub.status.idle": "2025-03-01T16:35:19.279826Z",
     "shell.execute_reply": "2025-03-01T16:35:19.278198Z",
     "shell.execute_reply.started": "2025-03-01T16:35:10.442092Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a9a123f910e47b3a539ab1b5323daa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.86k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a708bde03f5b44eead4a7b291c366acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877954f1c18d4d3589fd71e0c6d7e442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ddf4c15171d40fe9ad1ed2e34e1512c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 32102\n",
      "pad token id: <pad>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from tokenizers import AddedToken\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/t5-v1_1-base')\n",
    "tokenizer.add_tokens(AddedToken(\"\\n\", normalized=False))\n",
    "tokenizer.add_tokens(AddedToken(\"<s>\", normalized=False))\n",
    "vocab_size = len(tokenizer.get_vocab().keys())\n",
    "print(\"vocab_size:\", vocab_size)\n",
    "print(\"pad token id:\", tokenizer.pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T16:35:19.281979Z",
     "iopub.status.busy": "2025-03-01T16:35:19.281220Z",
     "iopub.status.idle": "2025-03-01T16:35:35.091453Z",
     "shell.execute_reply": "2025-03-01T16:35:35.089950Z",
     "shell.execute_reply.started": "2025-03-01T16:35:19.281940Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64846 32102\n",
      "9090\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.max_length = 2000000\n",
    "\n",
    "all_pos = {'PART', 'INTJ', 'SPACE', 'AUX', 'PUNCT', 'SYM', 'X', 'SCONJ', 'NUM', 'NOUN', 'ADP', 'ADJ', 'ADV', 'PRON', 'DET', 'CCONJ', 'PROPN', 'VERB'}\n",
    "#selected = {'NUM', 'NOUN', 'ADJ', 'PROPN'}  # For training\n",
    "selected = {'NUM', 'PROPN'}                  # For inference\n",
    "\n",
    "alltoks = sorted(list(tokenizer.get_vocab().items()), key=lambda x:x[1])\n",
    "all_toks_text = \"\\n\".join([t[0].replace(\"▁\", \"\") for t in alltoks])\n",
    "\n",
    "doc = nlp(all_toks_text)\n",
    "\n",
    "carry_toks = set()\n",
    "\n",
    "print(len(doc), len(alltoks))\n",
    "\n",
    "i = 0\n",
    "for ii, token in enumerate(doc):\n",
    "    if str(token) in alltoks[i][0]: pass\n",
    "    else: i += 1\n",
    "    if str(token) in alltoks[i][0] and token.pos_ in selected and i > 100:\n",
    "        if (token.pos_ != \"PROPN\" or alltoks[i][0].replace(\"▁\", \"\")[0].isupper()):\n",
    "            carry_toks.add(alltoks[i][1])\n",
    "print(len(carry_toks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T16:35:35.094615Z",
     "iopub.status.busy": "2025-03-01T16:35:35.093987Z",
     "iopub.status.idle": "2025-03-01T16:39:42.109002Z",
     "shell.execute_reply": "2025-03-01T16:39:42.108077Z",
     "shell.execute_reply.started": "2025-03-01T16:35:35.094586Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (531 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168881 150172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(\"dataset_rpc.json\", \"r\")\n",
    "data = json.loads(file.read())\n",
    "file.close()\n",
    "\n",
    "dataset = {}\n",
    "for subset in data:\n",
    "    dataset[subset] = {}\n",
    "    for subsubset in data[subset]:\n",
    "        dataset[subset][subsubset] = []\n",
    "        for text in data[subset][subsubset]:\n",
    "            text = \"\".join(text)\n",
    "            text = tokenizer.encode(\"<s>\" + text, add_special_tokens=False)\n",
    "            dataset[subset][subsubset].append(text)\n",
    "\n",
    "train = [text for data in dataset.values() for text in data[\"train\"]]\n",
    "test  = [text for data in dataset.values() for text in data[\"test\"]]\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T16:39:42.110461Z",
     "iopub.status.busy": "2025-03-01T16:39:42.110165Z",
     "iopub.status.idle": "2025-03-01T16:39:42.115389Z",
     "shell.execute_reply": "2025-03-01T16:39:42.113980Z",
     "shell.execute_reply.started": "2025-03-01T16:39:42.110437Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input_size  = 320 #512\n",
    "embed_dim   = 128\n",
    "not_carry_w = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T16:39:42.118196Z",
     "iopub.status.busy": "2025-03-01T16:39:42.117569Z",
     "iopub.status.idle": "2025-03-01T16:39:45.734621Z",
     "shell.execute_reply": "2025-03-01T16:39:45.732767Z",
     "shell.execute_reply.started": "2025-03-01T16:39:42.118138Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train = [text[:input_size+1] for text in train]\n",
    "train_padded = [text + ([tokenizer.pad_token_id] * (input_size+1 - len(text))) for text in train]\n",
    "\n",
    "test = [text[:input_size+1] for text in test]\n",
    "test_padded = [text + ([tokenizer.pad_token_id] * (input_size+1 - len(text))) for text in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights help the model suring training to focus on tokens like names, numbers and nouns that should be transported from the past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-18T17:31:47.702Z",
     "iopub.status.busy": "2025-02-18T17:22:34.704275Z",
     "iopub.status.idle": "2025-02-18T17:22:34.704614Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "weights = []\n",
    "\n",
    "for text in train:\n",
    "    in_past = set()\n",
    "    w = []\n",
    "    for i, t in enumerate(text):\n",
    "        if t in carry_toks:\n",
    "            if t in in_past:\n",
    "                w.append(1.0)\n",
    "            else:\n",
    "                in_past.add(t)\n",
    "                w.append(not_carry_w)\n",
    "        elif t != tokenizer.pad_token_id:\n",
    "            w.append(not_carry_w)\n",
    "        else: break\n",
    "    w += [0.0] * (input_size+1 - len(w))\n",
    "    weights.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-20T16:07:09.036506Z",
     "iopub.status.idle": "2025-02-20T16:07:09.037038Z",
     "shell.execute_reply": "2025-02-20T16:07:09.036811Z",
     "shell.execute_reply.started": "2025-02-20T16:07:09.036789Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X = tf.constant(train_padded, shape=(len(train_padded), input_size+1), dtype=tf.int32)\n",
    "T = tf.constant(test_padded,  shape=(len(test_padded),  input_size+1), dtype=tf.int32)\n",
    "W = tf.constant(weights,      shape=(len(weights),      input_size+1), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model\n",
    "Defining the embedding layer, differential attention layer and transformer model architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T16:39:45.740645Z",
     "iopub.status.busy": "2025-03-01T16:39:45.740193Z",
     "iopub.status.idle": "2025-03-01T16:39:45.748886Z",
     "shell.execute_reply": "2025-03-01T16:39:45.747419Z",
     "shell.execute_reply.started": "2025-03-01T16:39:45.740623Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def masked_accuracy(y_true, y_pred, padding_token=tokenizer.pad_token_id):\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "    y_pred = tf.cast(tf.argmax(y_pred, axis=-1), tf.int32)\n",
    "    mask = tf.cast(tf.not_equal(y_true, padding_token), tf.float32)\n",
    "    matches = tf.cast(tf.equal(y_true, y_pred), tf.float32)\n",
    "    accuracy = tf.reduce_sum(matches * mask) / tf.reduce_sum(mask)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T16:39:45.750946Z",
     "iopub.status.busy": "2025-03-01T16:39:45.750579Z",
     "iopub.status.idle": "2025-03-01T16:39:45.774773Z",
     "shell.execute_reply": "2025-03-01T16:39:45.772339Z",
     "shell.execute_reply.started": "2025-03-01T16:39:45.750918Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SharedEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embed_dim, **kwargs):\n",
    "        super(SharedEmbedding, self).__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.shared_weights = self.add_weight(\n",
    "            shape=(self.vocab_size, self.embed_dim),\n",
    "            initializer='random_normal',\n",
    "            trainable=True,\n",
    "            name='shared_weights'\n",
    "        )\n",
    "        super(SharedEmbedding, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs, mode='embedding'):\n",
    "        if mode == 'embedding':\n",
    "            return tf.nn.embedding_lookup(self.shared_weights, inputs)\n",
    "        elif mode == 'classify':\n",
    "            return tf.nn.softmax(tf.matmul(inputs, self.shared_weights, transpose_b=True), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T18:30:03.936125Z",
     "iopub.status.busy": "2025-03-01T18:30:03.935118Z",
     "iopub.status.idle": "2025-03-01T18:30:03.962479Z",
     "shell.execute_reply": "2025-03-01T18:30:03.961436Z",
     "shell.execute_reply.started": "2025-03-01T18:30:03.936090Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DiffAttention(keras.layers.Layer):\n",
    "    def __init__(self, depth, **kwargs):\n",
    "        super(DiffAttention, self).__init__(**kwargs)\n",
    "        self.lambda_init = 0.8 - 0.6 * math.exp(-0.3 * depth)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.embed_dim = input_shape[-1]\n",
    "        self.input_size = input_shape[-2]\n",
    "        self.mask = tf.where(tf.linalg.band_part(tf.ones((input_shape[-2], input_shape[-2])), -1, 0) == 1.0, 0.0, float(\"-inf\"))\n",
    "        self.range_do = -tf.range(input_shape[-2])-1\n",
    "        self.range_undo = tf.range(input_shape[-2])+1\n",
    "        self.Q = self.add_weight(name='kernelQ',\n",
    "                                      shape=(input_shape[-1], input_shape[-1]),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        self.K = self.add_weight(name='kernelK',\n",
    "                                      shape=(input_shape[-1], input_shape[-1]),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        self.V = self.add_weight(name='kernelV',\n",
    "                                      shape=(input_shape[-1], input_shape[-1]),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "\n",
    "        initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.1)\n",
    "        self.lambda_q1 = self.add_weight(\n",
    "            shape=(input_shape[-1],), initializer=initializer, trainable=True, name=\"lambda_q1\"\n",
    "        )\n",
    "        self.lambda_k1 = self.add_weight(\n",
    "            shape=(input_shape[-1],), initializer=initializer, trainable=True, name=\"lambda_k1\"\n",
    "        )\n",
    "        self.lambda_q2 = self.add_weight(\n",
    "            shape=(input_shape[-1],), initializer=initializer, trainable=True, name=\"lambda_q2\"\n",
    "        )\n",
    "        self.lambda_k2 = self.add_weight(\n",
    "            shape=(input_shape[-1],), initializer=initializer, trainable=True, name=\"lambda_k2\"\n",
    "        )\n",
    "        \n",
    "        super(DiffAttention, self).build(input_shape)\n",
    "\n",
    "    def roll_embeddings(self, tensor, shift_values):\n",
    "        batch_size, time_size, embed_dim = tensor.shape\n",
    "        if batch_size is None: return tensor\n",
    "        shift_matrix   = tf.reshape(shift_values, (1, -1, 1))\n",
    "        shift_matrix   = tf.tile(shift_matrix, [batch_size, 1, embed_dim])\n",
    "        indices        = tf.range(embed_dim)\n",
    "        indices_matrix = tf.tile(indices, [batch_size * time_size])\n",
    "        indices_matrix = tf.reshape(indices_matrix, (batch_size, time_size, embed_dim))\n",
    "        new_indices    = (indices_matrix + shift_matrix) % embed_dim     \n",
    "        rolled_tensor  = tf.gather(tensor, new_indices, batch_dims=2)\n",
    "        return rolled_tensor\n",
    "\n",
    "    def call(self, x, pos):\n",
    "        v    = x @ self.V\n",
    "        q    = tf.transpose(tf.reshape(x @ self.Q, (-1, self.input_size, 2, self.embed_dim//2)), perm=[0, 2, 1, 3])\n",
    "        k    = tf.transpose(tf.reshape(x @ self.K, (-1, self.input_size, 2, self.embed_dim//2)), perm=[0, 2, 1, 3])\n",
    "        atti = tf.matmul(q, k,   transpose_b=True)\n",
    "        attp = tf.matmul(q, pos, transpose_b=True)\n",
    "        attp = self.roll_embeddings(tf.reshape(attp, (-1, self.input_size, self.input_size)), self.range_do)\n",
    "        attp = tf.reshape(attp, (-1, 2, self.input_size, self.input_size))\n",
    "        att  = atti + attp\n",
    "        att  = tf.nn.softmax((att / math.sqrt(self.embed_dim)) + self.mask, axis=-1)\n",
    "        att1 = att[:, 0]\n",
    "        att2 = att[:, 1]\n",
    "        \n",
    "        lambda_1 = tf.math.exp(tf.reduce_sum(self.lambda_q1 * self.lambda_k1, axis=-1))\n",
    "        lambda_2 = tf.math.exp(tf.reduce_sum(self.lambda_q2 * self.lambda_k2, axis=-1))\n",
    "        lambda_full = lambda_1 - lambda_2 + self.lambda_init\n",
    "        att = att1 - lambda_full * att2\n",
    "\n",
    "        out = att @ v\n",
    "        out = out * (1 - self.lambda_init)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-20T17:16:56.464863Z",
     "iopub.status.busy": "2025-02-20T17:16:56.464301Z",
     "iopub.status.idle": "2025-02-20T17:16:59.161329Z",
     "shell.execute_reply": "2025-02-20T17:16:59.160222Z",
     "shell.execute_reply.started": "2025-02-20T17:16:56.464822Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(input_size, ), dtype=tf.int32)\n",
    "emb_layer = SharedEmbedding(vocab_size, embed_dim)\n",
    "pos_layer = keras_nlp.layers.PositionEmbedding(input_size)\n",
    "\n",
    "ins = LayerNormalization()(emb_layer(inputs, mode=\"embedding\"))\n",
    "x = ins\n",
    "pos_src = pos_layer(x)\n",
    "pos = tf.transpose(tf.reshape(pos_src, (-1, input_size, 2, embed_dim//2)), perm=[0, 2, 1, 3])\n",
    "\n",
    "b = 12\n",
    "for d in range(b):\n",
    "    x += (2*b)**-0.5 * LayerNormalization()(DiffAttention(d+1)(x, pos))\n",
    "    x1 = Dense(embed_dim, activation=\"gelu\")(x)\n",
    "    x1 = Dense(embed_dim, activation=\"gelu\")(x1)\n",
    "    x += (2*b)**-0.5 * LayerNormalization()(x1)\n",
    "\n",
    "x = emb_layer(x, mode=\"classify\")\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=x)\n",
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(ignore_class=tokenizer.pad_token_id),\n",
    "    optimizer=keras.optimizers.AdamW(learning_rate=0.001),\n",
    "    metrics=[masked_accuracy, keras_nlp.metrics.Perplexity(mask_token_id=tokenizer.pad_token_id)],\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-18T17:31:47.704Z",
     "iopub.status.busy": "2025-02-18T17:22:34.715840Z",
     "iopub.status.idle": "2025-02-18T17:22:34.716141Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for i in range(30):\n",
    "    \n",
    "    if i % 2 == 1 and i < 20:\n",
    "        w = tf.where(W < 1.0, 0.05, 1.0) \n",
    "    else:\n",
    "        w = tf.where(W < 1.0, 1.0, 1.0)\n",
    "    \n",
    "    model.fit(\n",
    "        x=X[:, :-1],\n",
    "        y=X[:, 1:],\n",
    "        shuffle=True,\n",
    "        epochs=1,\n",
    "        batch_size=60,\n",
    "        sample_weight=w[:, 1:]\n",
    "    )\n",
    "    \n",
    "    model.save(\"rpc.keras\")\n",
    "    \n",
    "    print(f\"Epoch {i+1} completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model\n",
    "Loading the model from file and creating helper function to vectorize texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T18:30:13.899505Z",
     "iopub.status.busy": "2025-03-01T18:30:13.899163Z",
     "iopub.status.idle": "2025-03-01T18:30:23.881753Z",
     "shell.execute_reply": "2025-03-01T18:30:23.880020Z",
     "shell.execute_reply.started": "2025-03-01T18:30:13.899484Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 320)]                0         []                            \n",
      "                                                                                                  \n",
      " shared_embedding (SharedEm  multiple                     4109056   ['input_1[0][0]']             \n",
      " bedding)                                                                                         \n",
      "                                                                                                  \n",
      " layer_normalization (Layer  (None, 320, 128)             256       ['shared_embedding[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " position_embedding (Positi  (None, 320, 128)             40960     ['layer_normalization[0][0]'] \n",
      " onEmbedding)                                                                                     \n",
      "                                                                                                  \n",
      " tf.reshape (TFOpLambda)     (None, 320, 2, 64)           0         ['position_embedding[0][0]']  \n",
      "                                                                                                  \n",
      " tf.compat.v1.transpose (TF  (None, 2, 320, 64)           0         ['tf.reshape[0][0]']          \n",
      " OpLambda)                                                                                        \n",
      "                                                                                                  \n",
      " diff_attention (DiffAttent  (None, 320, 128)             49664     ['layer_normalization[0][0]', \n",
      " ion)                                                                'tf.compat.v1.transpose[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'position_embedding[0][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_1 (Lay  (None, 320, 128)             256       ['diff_attention[0][0]']      \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLamb  (None, 320, 128)             0         ['layer_normalization_1[0][0]'\n",
      " da)                                                                ]                             \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOp  (None, 320, 128)             0         ['layer_normalization[0][0]', \n",
      " Lambda)                                                             'tf.math.multiply[0][0]']    \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 320, 128)             16512     ['tf.__operators__.add[0][0]']\n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 320, 128)             16512     ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_2 (Lay  (None, 320, 128)             256       ['dense_1[0][0]']             \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " tf.math.multiply_1 (TFOpLa  (None, 320, 128)             0         ['layer_normalization_2[0][0]'\n",
      " mbda)                                                              ]                             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TF  (None, 320, 128)             0         ['tf.__operators__.add[0][0]',\n",
      " OpLambda)                                                           'tf.math.multiply_1[0][0]']  \n",
      "                                                                                                  \n",
      " diff_attention_1 (DiffAtte  (None, 320, 128)             49664     ['tf.__operators__.add_1[0][0]\n",
      " ntion)                                                             ',                            \n",
      "                                                                     'tf.compat.v1.transpose[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'position_embedding[0][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_3 (Lay  (None, 320, 128)             256       ['diff_attention_1[0][0]']    \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " tf.math.multiply_2 (TFOpLa  (None, 320, 128)             0         ['layer_normalization_3[0][0]'\n",
      " mbda)                                                              ]                             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TF  (None, 320, 128)             0         ['tf.__operators__.add_1[0][0]\n",
      " OpLambda)                                                          ',                            \n",
      "                                                                     'tf.math.multiply_2[0][0]']  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 320, 128)             16512     ['tf.__operators__.add_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 320, 128)             16512     ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_4 (Lay  (None, 320, 128)             256       ['dense_3[0][0]']             \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " tf.math.multiply_3 (TFOpLa  (None, 320, 128)             0         ['layer_normalization_4[0][0]'\n",
      " mbda)                                                              ]                             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TF  (None, 320, 128)             0         ['tf.__operators__.add_2[0][0]\n",
      " OpLambda)                                                          ',                            \n",
      "                                                                     'tf.math.multiply_3[0][0]']  \n",
      "                                                                                                  \n",
      " diff_attention_2 (DiffAtte  (None, 320, 128)             49664     ['tf.__operators__.add_3[0][0]\n",
      " ntion)                                                             ',                            \n",
      "                                                                     'tf.compat.v1.transpose[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'position_embedding[0][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_5 (Lay  (None, 320, 128)             256       ['diff_attention_2[0][0]']    \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " tf.math.multiply_4 (TFOpLa  (None, 320, 128)             0         ['layer_normalization_5[0][0]'\n",
      " mbda)                                                              ]                             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TF  (None, 320, 128)             0         ['tf.__operators__.add_3[0][0]\n",
      " OpLambda)                                                          ',                            \n",
      "                                                                     'tf.math.multiply_4[0][0]']  \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 320, 128)             16512     ['tf.__operators__.add_4[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 320, 128)             16512     ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_6 (Lay  (None, 320, 128)             256       ['dense_5[0][0]']             \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " tf.math.multiply_5 (TFOpLa  (None, 320, 128)             0         ['layer_normalization_6[0][0]'\n",
      " mbda)                                                              ]                             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TF  (None, 320, 128)             0         ['tf.__operators__.add_4[0][0]\n",
      " OpLambda)                                                          ',                            \n",
      "                                                                     'tf.math.multiply_5[0][0]']  \n",
      "                                                                                                  \n",
      " diff_attention_3 (DiffAtte  (None, 320, 128)             49664     ['tf.__operators__.add_5[0][0]\n",
      " ntion)                                                             ',                            \n",
      "                                                                     'tf.compat.v1.transpose[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'position_embedding[0][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_7 (Lay  (None, 320, 128)             256       ['diff_attention_3[0][0]']    \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " tf.math.multiply_6 (TFOpLa  (None, 320, 128)             0         ['layer_normalization_7[0][0]'\n",
      " mbda)                                                              ]                             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TF  (None, 320, 128)             0         ['tf.__operators__.add_5[0][0]\n",
      " OpLambda)                                                          ',                            \n",
      "                                                                     'tf.math.multiply_6[0][0]']  \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 320, 128)             16512     ['tf.__operators__.add_6[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 320, 128)             16512     ['dense_6[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_8 (Lay  (None, 320, 128)             256       ['dense_7[0][0]']             \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " tf.math.multiply_7 (TFOpLa  (None, 320, 128)             0         ['layer_normalization_8[0][0]'\n",
      " mbda)                                                              ]                             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TF  (None, 320, 128)             0         ['tf.__operators__.add_6[0][0]\n",
      " OpLambda)                                                          ',                            \n",
      "                                                                     'tf.math.multiply_7[0][0]']  \n",
      "                                                                                                  \n",
      " diff_attention_4 (DiffAtte  (None, 320, 128)             49664     ['tf.__operators__.add_7[0][0]\n",
      " ntion)                                                             ',                            \n",
      "                                                                     'tf.compat.v1.transpose[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'position_embedding[0][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_9 (Lay  (None, 320, 128)             256       ['diff_attention_4[0][0]']    \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " tf.math.multiply_8 (TFOpLa  (None, 320, 128)             0         ['layer_normalization_9[0][0]'\n",
      " mbda)                                                              ]                             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_8 (TF  (None, 320, 128)             0         ['tf.__operators__.add_7[0][0]\n",
      " OpLambda)                                                          ',                            \n",
      "                                                                     'tf.math.multiply_8[0][0]']  \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, 320, 128)             16512     ['tf.__operators__.add_8[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_9 (Dense)             (None, 320, 128)             16512     ['dense_8[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_10 (La  (None, 320, 128)             256       ['dense_9[0][0]']             \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_9 (TFOpLa  (None, 320, 128)             0         ['layer_normalization_10[0][0]\n",
      " mbda)                                                              ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_9 (TF  (None, 320, 128)             0         ['tf.__operators__.add_8[0][0]\n",
      " OpLambda)                                                          ',                            \n",
      "                                                                     'tf.math.multiply_9[0][0]']  \n",
      "                                                                                                  \n",
      " diff_attention_5 (DiffAtte  (None, 320, 128)             49664     ['tf.__operators__.add_9[0][0]\n",
      " ntion)                                                             ',                            \n",
      "                                                                     'tf.compat.v1.transpose[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'position_embedding[0][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_11 (La  (None, 320, 128)             256       ['diff_attention_5[0][0]']    \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_10 (TFOpL  (None, 320, 128)             0         ['layer_normalization_11[0][0]\n",
      " ambda)                                                             ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_10 (T  (None, 320, 128)             0         ['tf.__operators__.add_9[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'tf.math.multiply_10[0][0]'] \n",
      "                                                                                                  \n",
      " dense_10 (Dense)            (None, 320, 128)             16512     ['tf.__operators__.add_10[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dense_11 (Dense)            (None, 320, 128)             16512     ['dense_10[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_12 (La  (None, 320, 128)             256       ['dense_11[0][0]']            \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_11 (TFOpL  (None, 320, 128)             0         ['layer_normalization_12[0][0]\n",
      " ambda)                                                             ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_11 (T  (None, 320, 128)             0         ['tf.__operators__.add_10[0][0\n",
      " FOpLambda)                                                         ]',                           \n",
      "                                                                     'tf.math.multiply_11[0][0]'] \n",
      "                                                                                                  \n",
      " diff_attention_6 (DiffAtte  (None, 320, 128)             49664     ['tf.__operators__.add_11[0][0\n",
      " ntion)                                                             ]',                           \n",
      "                                                                     'tf.compat.v1.transpose[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'position_embedding[0][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_13 (La  (None, 320, 128)             256       ['diff_attention_6[0][0]']    \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_12 (TFOpL  (None, 320, 128)             0         ['layer_normalization_13[0][0]\n",
      " ambda)                                                             ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_12 (T  (None, 320, 128)             0         ['tf.__operators__.add_11[0][0\n",
      " FOpLambda)                                                         ]',                           \n",
      "                                                                     'tf.math.multiply_12[0][0]'] \n",
      "                                                                                                  \n",
      " dense_12 (Dense)            (None, 320, 128)             16512     ['tf.__operators__.add_12[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dense_13 (Dense)            (None, 320, 128)             16512     ['dense_12[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_14 (La  (None, 320, 128)             256       ['dense_13[0][0]']            \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_13 (TFOpL  (None, 320, 128)             0         ['layer_normalization_14[0][0]\n",
      " ambda)                                                             ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_13 (T  (None, 320, 128)             0         ['tf.__operators__.add_12[0][0\n",
      " FOpLambda)                                                         ]',                           \n",
      "                                                                     'tf.math.multiply_13[0][0]'] \n",
      "                                                                                                  \n",
      " diff_attention_7 (DiffAtte  (None, 320, 128)             49664     ['tf.__operators__.add_13[0][0\n",
      " ntion)                                                             ]',                           \n",
      "                                                                     'tf.compat.v1.transpose[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'position_embedding[0][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_15 (La  (None, 320, 128)             256       ['diff_attention_7[0][0]']    \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_14 (TFOpL  (None, 320, 128)             0         ['layer_normalization_15[0][0]\n",
      " ambda)                                                             ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_14 (T  (None, 320, 128)             0         ['tf.__operators__.add_13[0][0\n",
      " FOpLambda)                                                         ]',                           \n",
      "                                                                     'tf.math.multiply_14[0][0]'] \n",
      "                                                                                                  \n",
      " dense_14 (Dense)            (None, 320, 128)             16512     ['tf.__operators__.add_14[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dense_15 (Dense)            (None, 320, 128)             16512     ['dense_14[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_16 (La  (None, 320, 128)             256       ['dense_15[0][0]']            \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_15 (TFOpL  (None, 320, 128)             0         ['layer_normalization_16[0][0]\n",
      " ambda)                                                             ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_15 (T  (None, 320, 128)             0         ['tf.__operators__.add_14[0][0\n",
      " FOpLambda)                                                         ]',                           \n",
      "                                                                     'tf.math.multiply_15[0][0]'] \n",
      "                                                                                                  \n",
      " diff_attention_8 (DiffAtte  (None, 320, 128)             49664     ['tf.__operators__.add_15[0][0\n",
      " ntion)                                                             ]',                           \n",
      "                                                                     'tf.compat.v1.transpose[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'position_embedding[0][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_17 (La  (None, 320, 128)             256       ['diff_attention_8[0][0]']    \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_16 (TFOpL  (None, 320, 128)             0         ['layer_normalization_17[0][0]\n",
      " ambda)                                                             ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_16 (T  (None, 320, 128)             0         ['tf.__operators__.add_15[0][0\n",
      " FOpLambda)                                                         ]',                           \n",
      "                                                                     'tf.math.multiply_16[0][0]'] \n",
      "                                                                                                  \n",
      " dense_16 (Dense)            (None, 320, 128)             16512     ['tf.__operators__.add_16[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dense_17 (Dense)            (None, 320, 128)             16512     ['dense_16[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_18 (La  (None, 320, 128)             256       ['dense_17[0][0]']            \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_17 (TFOpL  (None, 320, 128)             0         ['layer_normalization_18[0][0]\n",
      " ambda)                                                             ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_17 (T  (None, 320, 128)             0         ['tf.__operators__.add_16[0][0\n",
      " FOpLambda)                                                         ]',                           \n",
      "                                                                     'tf.math.multiply_17[0][0]'] \n",
      "                                                                                                  \n",
      " diff_attention_9 (DiffAtte  (None, 320, 128)             49664     ['tf.__operators__.add_17[0][0\n",
      " ntion)                                                             ]',                           \n",
      "                                                                     'tf.compat.v1.transpose[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'position_embedding[0][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_19 (La  (None, 320, 128)             256       ['diff_attention_9[0][0]']    \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_18 (TFOpL  (None, 320, 128)             0         ['layer_normalization_19[0][0]\n",
      " ambda)                                                             ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_18 (T  (None, 320, 128)             0         ['tf.__operators__.add_17[0][0\n",
      " FOpLambda)                                                         ]',                           \n",
      "                                                                     'tf.math.multiply_18[0][0]'] \n",
      "                                                                                                  \n",
      " dense_18 (Dense)            (None, 320, 128)             16512     ['tf.__operators__.add_18[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dense_19 (Dense)            (None, 320, 128)             16512     ['dense_18[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_20 (La  (None, 320, 128)             256       ['dense_19[0][0]']            \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_19 (TFOpL  (None, 320, 128)             0         ['layer_normalization_20[0][0]\n",
      " ambda)                                                             ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_19 (T  (None, 320, 128)             0         ['tf.__operators__.add_18[0][0\n",
      " FOpLambda)                                                         ]',                           \n",
      "                                                                     'tf.math.multiply_19[0][0]'] \n",
      "                                                                                                  \n",
      " diff_attention_10 (DiffAtt  (None, 320, 128)             49664     ['tf.__operators__.add_19[0][0\n",
      " ention)                                                            ]',                           \n",
      "                                                                     'tf.compat.v1.transpose[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'position_embedding[0][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_21 (La  (None, 320, 128)             256       ['diff_attention_10[0][0]']   \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_20 (TFOpL  (None, 320, 128)             0         ['layer_normalization_21[0][0]\n",
      " ambda)                                                             ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_20 (T  (None, 320, 128)             0         ['tf.__operators__.add_19[0][0\n",
      " FOpLambda)                                                         ]',                           \n",
      "                                                                     'tf.math.multiply_20[0][0]'] \n",
      "                                                                                                  \n",
      " dense_20 (Dense)            (None, 320, 128)             16512     ['tf.__operators__.add_20[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dense_21 (Dense)            (None, 320, 128)             16512     ['dense_20[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_22 (La  (None, 320, 128)             256       ['dense_21[0][0]']            \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_21 (TFOpL  (None, 320, 128)             0         ['layer_normalization_22[0][0]\n",
      " ambda)                                                             ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_21 (T  (None, 320, 128)             0         ['tf.__operators__.add_20[0][0\n",
      " FOpLambda)                                                         ]',                           \n",
      "                                                                     'tf.math.multiply_21[0][0]'] \n",
      "                                                                                                  \n",
      " diff_attention_11 (DiffAtt  (None, 320, 128)             49664     ['tf.__operators__.add_21[0][0\n",
      " ention)                                                            ]',                           \n",
      "                                                                     'tf.compat.v1.transpose[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'position_embedding[0][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_23 (La  (None, 320, 128)             256       ['diff_attention_11[0][0]']   \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_22 (TFOpL  (None, 320, 128)             0         ['layer_normalization_23[0][0]\n",
      " ambda)                                                             ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_22 (T  (None, 320, 128)             0         ['tf.__operators__.add_21[0][0\n",
      " FOpLambda)                                                         ]',                           \n",
      "                                                                     'tf.math.multiply_22[0][0]'] \n",
      "                                                                                                  \n",
      " dense_22 (Dense)            (None, 320, 128)             16512     ['tf.__operators__.add_22[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dense_23 (Dense)            (None, 320, 128)             16512     ['dense_22[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_24 (La  (None, 320, 128)             256       ['dense_23[0][0]']            \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_23 (TFOpL  (None, 320, 128)             0         ['layer_normalization_24[0][0]\n",
      " ambda)                                                             ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_23 (T  (None, 320, 128)             0         ['tf.__operators__.add_22[0][0\n",
      " FOpLambda)                                                         ]',                           \n",
      "                                                                     'tf.math.multiply_23[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5148672 (19.64 MB)\n",
      "Trainable params: 5148672 (19.64 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(\n",
    "    \"rpc.keras\",\n",
    "    custom_objects={\n",
    "        \"DiffAttention\" : DiffAttention,\n",
    "        \"SharedEmbedding\" : SharedEmbedding,\n",
    "        \"masked_accuracy\" : masked_accuracy\n",
    "    }\n",
    ")\n",
    "encoder = keras.Model(inputs=model.layers[0].input, outputs=model.layers[-1].output)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T18:30:23.889245Z",
     "iopub.status.busy": "2025-03-01T18:30:23.888858Z",
     "iopub.status.idle": "2025-03-01T18:30:25.795027Z",
     "shell.execute_reply": "2025-03-01T18:30:25.794212Z",
     "shell.execute_reply.started": "2025-03-01T18:30:23.889202Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.47240964,  0.01392975, -0.5449621 , ..., -0.63921344,\n",
       "         0.4831988 ,  2.2904954 ],\n",
       "       [-2.1884563 ,  2.0251868 ,  0.04233116, ..., -0.15716928,\n",
       "         1.1762643 ,  1.8324168 ],\n",
       "       [-1.8713562 , -0.9782971 ,  1.1051077 , ...,  0.05921268,\n",
       "         1.6744466 ,  1.8488976 ],\n",
       "       ...,\n",
       "       [-1.6971186 ,  4.2969613 ,  2.8973937 , ..., -2.325593  ,\n",
       "         0.2113786 , -0.94676805],\n",
       "       [-1.9779043 ,  0.9131648 ,  1.5120801 , ..., -1.4387566 ,\n",
       "        -1.0139507 ,  3.700141  ],\n",
       "       [-1.4337753 ,  0.7190973 ,  0.82540727, ..., -1.3526881 ,\n",
       "         2.563802  ,  1.6192843 ]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vectorize_texts(all_texts):\n",
    "    batch_size = 128\n",
    "    vects = []\n",
    "    for i in range(0, len(all_texts), batch_size):\n",
    "        texts = all_texts[i:i+batch_size]\n",
    "        toks = [text + ([tokenizer.pad_token_id] * (input_size - len(text))) for text in texts]\n",
    "        if len(toks) > 0:\n",
    "            toks = tf.constant(toks, shape=(len(toks), input_size))\n",
    "            vect = encoder.predict(toks, verbose=0)\n",
    "            for v, t in zip(vect, texts):\n",
    "                vects.append(v[:len(t), :])\n",
    "    return tf.concat(vects, axis=0).numpy()\n",
    "\n",
    "vectorize_texts([\n",
    "    tokenizer.encode(\"<s>Hello there. how are you?\", add_special_tokens=False),\n",
    "    tokenizer.encode(\"<s>Hello there. how have you been?\", add_special_tokens=False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NGT Based Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T14:16:25.932211Z",
     "iopub.status.busy": "2025-02-28T14:16:25.931763Z",
     "iopub.status.idle": "2025-02-28T14:19:56.339029Z",
     "shell.execute_reply": "2025-02-28T14:19:56.337419Z",
     "shell.execute_reply.started": "2025-02-28T14:16:25.932180Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jpmag\\Downloads\\NGT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'NGT'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jpmag\\Downloads\\NGT\\build\n",
      "-- The C compiler identification is GNU 11.2.0\n",
      "-- The CXX compiler identification is GNU 11.2.0\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - failed\n",
      "-- Check for working C compiler: /usr/bin/cc\n",
      "-- Check for working C compiler: /usr/bin/cc - works\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - failed\n",
      "-- Check for working CXX compiler: /usr/bin/c++.exe\n",
      "-- Check for working CXX compiler: /usr/bin/c++.exe - works\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- VERSION: 2.2.4\n",
      "-- CMAKE_BUILD_TYPE: Release\n",
      "-- CMAKE_BUILD_TYPE_LOWER: release\n",
      "-- Configuring incomplete, errors occurred!\n",
      "See also \"/cygdrive/c/Users/jpmag/Downloads/NGT/build/CMakeFiles/CMakeOutput.log\".\n",
      "See also \"/cygdrive/c/Users/jpmag/Downloads/NGT/build/CMakeFiles/CMakeError.log\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CMake Deprecation Warning at CMakeLists.txt:4 (cmake_minimum_required):\n",
      "  Compatibility with CMake < 2.8.12 will be removed from a future version of\n",
      "  CMake.\n",
      "\n",
      "  Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
      "  CMake that the project does not need compatibility with older versions.\n",
      "\n",
      "\n",
      "CMake Warning at /usr/share/cmake-3.20.0/Modules/Platform/CYGWIN.cmake:15 (message):\n",
      "  CMake no longer defines WIN32 on Cygwin!\n",
      "\n",
      "  (1) If you are just trying to build this project, ignore this warning or\n",
      "  quiet it by setting CMAKE_LEGACY_CYGWIN_WIN32=0 in your environment or in\n",
      "  the CMake cache.  If later configuration or build errors occur then this\n",
      "  project may have been written under the assumption that Cygwin is WIN32.\n",
      "  In that case, set CMAKE_LEGACY_CYGWIN_WIN32=1 instead.\n",
      "\n",
      "  (2) If you are developing this project, add the line\n",
      "\n",
      "    set(CMAKE_LEGACY_CYGWIN_WIN32 0) # Remove when CMake >= 2.8.4 is required\n",
      "\n",
      "  at the top of your top-level CMakeLists.txt file or set the minimum\n",
      "  required version of CMake to 2.8.4 or higher.  Then teach your project to\n",
      "  build on Cygwin without WIN32.\n",
      "Call Stack (most recent call first):\n",
      "  /usr/share/cmake-3.20.0/Modules/CMakeSystemSpecificInformation.cmake:26 (include)\n",
      "  CMakeLists.txt:7 (project)\n",
      "\n",
      "\n",
      "CMake Error at /usr/share/cmake-3.20.0/Modules/FindPackageHandleStandardArgs.cmake:230 (message):\n",
      "  Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES)\n",
      "Call Stack (most recent call first):\n",
      "  /usr/share/cmake-3.20.0/Modules/FindPackageHandleStandardArgs.cmake:594 (_FPHSA_FAILURE_MESSAGE)\n",
      "  /usr/share/cmake-3.20.0/Modules/FindOpenMP.cmake:542 (find_package_handle_standard_args)\n",
      "  CMakeLists.txt:81 (find_package)\n",
      "\n",
      "\n",
      "make: *** No targets specified and no makefile found.  Stop.\n",
      "make: *** No rule to make target 'install'.  Stop.\n",
      "'ldconfig' is not recognized as an internal or external command,"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 3] O sistema não conseguiu localizar o caminho especificado: '/kaggle/working/NGT/python'\n",
      "c:\\Users\\jpmag\\Downloads\\NGT\\build\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "operable program or batch file.\n",
      "Python n�o foi encontrado; executar sem argumentos para instalar a partir da Microsoft Store ou desativar este atalho a partir de Defini��es > Aplica��es > Defini��es avan�adas da aplica��o > Aliases de execu��o de aplica��es.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\users\\jpmag\\downloads\\ngt\\build\\dist\\ngt-2.2.4.tar.gz\n",
      "[WinError 3] O sistema não conseguiu localizar o caminho especificado: '/kaggle/working'\n",
      "c:\\Users\\jpmag\\Downloads\\NGT\\build\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Requirement 'dist/ngt-2.2.4.tar.gz' looks like a filename, but the file does not exist\n",
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\jpmag\\\\Downloads\\\\NGT\\\\build\\\\dist\\\\ngt-2.2.4.tar.gz'\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/jpmag7/NGT.git\n",
    "%cd NGT\n",
    "!mkdir build\n",
    "%cd build\n",
    "!cmake -DNGT_SHARED_MEMORY_ALLOCATOR=ON ..\n",
    "!make\n",
    "!make install\n",
    "!ldconfig /usr/local/lib\n",
    "%cd /kaggle/working/NGT/python\n",
    "!python3 setup.py sdist\n",
    "!pip3 install dist/ngt-2.2.4.tar.gz\n",
    "%cd /kaggle/working\n",
    "!rm -r NGT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T14:19:56.341947Z",
     "iopub.status.busy": "2025-02-28T14:19:56.341567Z",
     "iopub.status.idle": "2025-02-28T15:05:36.026365Z",
     "shell.execute_reply": "2025-02-28T15:05:36.024705Z",
     "shell.execute_reply.started": "2025-02-28T14:19:56.341911Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import ngtpy\n",
    "import json\n",
    "\n",
    "size = 30_000\n",
    "batch_size = 2048\n",
    "\n",
    "index_path = \"index\"\n",
    "ngtpy.create(index_path, embed_dim)\n",
    "index = ngtpy.Index(index_path)\n",
    "\n",
    "all_toks = []\n",
    "\n",
    "for start in tqdm(range(0, size, batch_size)):\n",
    "    \n",
    "    prompt_embeds = vectorize_texts([t[:-1] for t in train[start:min(size, start+batch_size)]])\n",
    "    \n",
    "    chars = [t for text in train[start:min(size, start+batch_size)] for t in text[1:]]\n",
    "    for c in chars: all_toks.append(c)\n",
    "\n",
    "    if prompt_embeds.shape[0] > 0: index.batch_insert(prompt_embeds)\n",
    "    \n",
    "with open(\"index/all_toks.json\", \"w\") as f:\n",
    "    f.write(json.dumps(all_toks))\n",
    "\n",
    "print(\"building objects...\")\n",
    "index.build_index()\n",
    "print(\"saving the index...\")\n",
    "index.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T11:01:02.803163Z",
     "iopub.status.busy": "2025-02-25T11:01:02.802609Z",
     "iopub.status.idle": "2025-02-25T11:01:03.597044Z",
     "shell.execute_reply": "2025-02-25T11:01:03.595800Z",
     "shell.execute_reply.started": "2025-02-25T11:01:02.803120Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "index_path = \"/kaggle/working/index\"\n",
    "index = ngtpy.Index(index_path, read_only=True)\n",
    "\n",
    "with open(\"all_toks.json\", \"r\") as f:\n",
    "    all_toks = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flat Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T19:11:54.499934Z",
     "iopub.status.busy": "2025-03-01T19:11:54.499554Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# random.seed(10)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# random.shuffle(train)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m60_000\u001b[39m\n\u001b[1;32m----> 8\u001b[0m all_toks \u001b[38;5;241m=\u001b[39m [t \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain\u001b[49m[:size] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m text[\u001b[38;5;241m1\u001b[39m:]]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(encoder\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshared_weights\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m     10\u001b[0m     all_toks\u001b[38;5;241m.\u001b[39mappend(i)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "size = 30_000\n",
    "\n",
    "all_toks = [t for text in train[:size] for t in text[1:]]\n",
    "with open(\"all_toks.json\", \"w\") as f:\n",
    "    f.write(json.dumps(all_toks))\n",
    "\n",
    "embeds = []\n",
    "batch_size = 2048\n",
    "for start in tqdm(range(0, size, batch_size)):\n",
    "    prompt_embeds = vectorize_texts([t[:-1] for t in train[start:min(size, start+batch_size)]])\n",
    "    embeds.append(prompt_embeds)\n",
    "embeds = tf.concat(embeds, axis=0)\n",
    "\n",
    "import faiss\n",
    "index = faiss.IndexFlatL2(embed_dim)\n",
    "index.add(embeds)\n",
    "faiss.write_index(index, \"index.faiss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T19:10:54.343662Z",
     "iopub.status.busy": "2025-03-01T19:10:54.343320Z",
     "iopub.status.idle": "2025-03-01T19:11:37.482149Z",
     "shell.execute_reply": "2025-03-01T19:11:37.480819Z",
     "shell.execute_reply.started": "2025-03-01T19:10:54.343640Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1> Peter: Open the garage door\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " John: What's it like?\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17> Peter: its a door like any other\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " John: Yeah, it does!\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35> Peter: What the fuck are you even\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " John: I'm sorry, I won't do it again.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m sents \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 4\u001b[0m     user \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43menc_text\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m     user \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(user, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m     sents\u001b[38;5;241m.\u001b[39mappend(user)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "enc_text = tokenizer.encode(\"<s>\", add_special_tokens=False)\n",
    "sents = []\n",
    "while True:\n",
    "    user = input(f\"{len(enc_text)}>\") + \"\\n\"\n",
    "    user = tokenizer.encode(user, add_special_tokens=False)\n",
    "    sents.append(user)\n",
    "    enc_text += user\n",
    "    new_text = tokenizer.decode(enc_text)\n",
    "    text = new_text\n",
    "    tok = 0\n",
    "    sents.append([])\n",
    "    while tok != vocab_size - 2:\n",
    "        xq = vectorize_texts([enc_text])[-1]\n",
    "\n",
    "        # If using faiss index\n",
    "        _id = index.search(xq.reshape((1, -1)), 1)[1][0][0]\n",
    "        \n",
    "        # If using ngt index\n",
    "        #_id = index.search(xq, size=1, epsilon=1)[0][0]\n",
    "        \n",
    "        if all_toks[_id] in carry_toks:\n",
    "            tmp = tf.argmax(tf.matmul(xq.reshape((1, -1)), encoder.layers[1].shared_weights, transpose_b=True), axis=-1).numpy()[0]\n",
    "            if tmp in enc_text: tok = tmp\n",
    "            else: tok = all_toks[_id]\n",
    "        else:\n",
    "            tok = all_toks[_id]\n",
    "\n",
    "        sents[-1].append(tok)\n",
    "        enc_text += [tok]\n",
    "        new_text = tokenizer.decode(enc_text)\n",
    "        print(new_text[len(text):], end=\"\")\n",
    "        text = new_text\n",
    "    print(\"\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11258714,
     "datasetId": 4261095,
     "sourceId": 10890851,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 11251702,
     "datasetId": 5560278,
     "sourceId": 10884533,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
