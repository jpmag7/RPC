{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-01T16:34:06.041102Z",
     "iopub.status.busy": "2025-03-01T16:34:06.040783Z",
     "iopub.status.idle": "2025-03-01T16:34:49.236547Z",
     "shell.execute_reply": "2025-03-01T16:34:49.234814Z",
     "shell.execute_reply.started": "2025-03-01T16:34:06.041075Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install keras-nlp==0.10.0\n",
    "!pip install keras==2.15.0\n",
    "!pip install tensorflow==2.15.0\n",
    "!pip install faiss-cpu==1.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T16:34:49.239070Z",
     "iopub.status.busy": "2025-03-01T16:34:49.238731Z",
     "iopub.status.idle": "2025-03-01T16:35:10.440457Z",
     "shell.execute_reply": "2025-03-01T16:35:10.439415Z",
     "shell.execute_reply.started": "2025-03-01T16:34:49.239040Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import *\n",
    "import keras_nlp\n",
    "from keras import backend\n",
    "\n",
    "import re\n",
    "import requests\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import string\n",
    "import nltk\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T16:35:10.442117Z",
     "iopub.status.busy": "2025-03-01T16:35:10.441571Z",
     "iopub.status.idle": "2025-03-01T16:35:19.279826Z",
     "shell.execute_reply": "2025-03-01T16:35:19.278198Z",
     "shell.execute_reply.started": "2025-03-01T16:35:10.442092Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from tokenizers import AddedToken\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/t5-v1_1-base')\n",
    "tokenizer.add_tokens(AddedToken(\"\\n\", normalized=False))\n",
    "tokenizer.add_tokens(AddedToken(\"<s>\", normalized=False))\n",
    "vocab_size = len(tokenizer.get_vocab().keys())\n",
    "print(\"vocab_size:\", vocab_size)\n",
    "print(\"pad token id:\", tokenizer.pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T16:35:19.281979Z",
     "iopub.status.busy": "2025-03-01T16:35:19.281220Z",
     "iopub.status.idle": "2025-03-01T16:35:35.091453Z",
     "shell.execute_reply": "2025-03-01T16:35:35.089950Z",
     "shell.execute_reply.started": "2025-03-01T16:35:19.281940Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.max_length = 2000000\n",
    "\n",
    "all_pos = {'PART', 'INTJ', 'SPACE', 'AUX', 'PUNCT', 'SYM', 'X', 'SCONJ', 'NUM', 'NOUN', 'ADP', 'ADJ', 'ADV', 'PRON', 'DET', 'CCONJ', 'PROPN', 'VERB'}\n",
    "#selected = {'NUM', 'NOUN', 'ADJ', 'PROPN'}  # For training\n",
    "selected = {'NUM', 'PROPN'}                  # For inference\n",
    "\n",
    "alltoks = sorted(list(tokenizer.get_vocab().items()), key=lambda x:x[1])\n",
    "all_toks_text = \"\\n\".join([t[0].replace(\"▁\", \"\") for t in alltoks])\n",
    "\n",
    "doc = nlp(all_toks_text)\n",
    "\n",
    "carry_toks = set()\n",
    "\n",
    "print(len(doc), len(alltoks))\n",
    "\n",
    "i = 0\n",
    "for ii, token in enumerate(doc):\n",
    "    if str(token) in alltoks[i][0]: pass\n",
    "    else: i += 1\n",
    "    if str(token) in alltoks[i][0] and token.pos_ in selected and i > 100:\n",
    "        if (token.pos_ != \"PROPN\" or alltoks[i][0].replace(\"▁\", \"\")[0].isupper()):\n",
    "            carry_toks.add(alltoks[i][1])\n",
    "print(len(carry_toks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T16:35:35.094615Z",
     "iopub.status.busy": "2025-03-01T16:35:35.093987Z",
     "iopub.status.idle": "2025-03-01T16:39:42.109002Z",
     "shell.execute_reply": "2025-03-01T16:39:42.108077Z",
     "shell.execute_reply.started": "2025-03-01T16:35:35.094586Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "file = open(\"dataset_rpc.json\", \"r\")\n",
    "data = json.loads(file.read())\n",
    "file.close()\n",
    "\n",
    "dataset = {}\n",
    "for subset in data:\n",
    "    dataset[subset] = {}\n",
    "    for subsubset in data[subset]:\n",
    "        dataset[subset][subsubset] = []\n",
    "        for text in data[subset][subsubset]:\n",
    "            text = \"\".join(text)\n",
    "            text = tokenizer.encode(\"<s>\" + text, add_special_tokens=False)\n",
    "            dataset[subset][subsubset].append(text)\n",
    "\n",
    "train = [text for data in dataset.values() for text in data[\"train\"]]\n",
    "test  = [text for data in dataset.values() for text in data[\"test\"]]\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T16:39:42.110461Z",
     "iopub.status.busy": "2025-03-01T16:39:42.110165Z",
     "iopub.status.idle": "2025-03-01T16:39:42.115389Z",
     "shell.execute_reply": "2025-03-01T16:39:42.113980Z",
     "shell.execute_reply.started": "2025-03-01T16:39:42.110437Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input_size  = 320 #512\n",
    "embed_dim   = 128\n",
    "not_carry_w = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T16:39:42.118196Z",
     "iopub.status.busy": "2025-03-01T16:39:42.117569Z",
     "iopub.status.idle": "2025-03-01T16:39:45.734621Z",
     "shell.execute_reply": "2025-03-01T16:39:45.732767Z",
     "shell.execute_reply.started": "2025-03-01T16:39:42.118138Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train = [text[:input_size+1] for text in train]\n",
    "train_padded = [text + ([tokenizer.pad_token_id] * (input_size+1 - len(text))) for text in train]\n",
    "\n",
    "test = [text[:input_size+1] for text in test]\n",
    "test_padded = [text + ([tokenizer.pad_token_id] * (input_size+1 - len(text))) for text in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights help the model suring training to focus on tokens like names, numbers and nouns that should be transported from the past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-18T17:31:47.702Z",
     "iopub.status.busy": "2025-02-18T17:22:34.704275Z",
     "iopub.status.idle": "2025-02-18T17:22:34.704614Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "weights = []\n",
    "\n",
    "for text in train:\n",
    "    in_past = set()\n",
    "    w = []\n",
    "    for i, t in enumerate(text):\n",
    "        if t in carry_toks:\n",
    "            if t in in_past:\n",
    "                w.append(1.0)\n",
    "            else:\n",
    "                in_past.add(t)\n",
    "                w.append(not_carry_w)\n",
    "        elif t != tokenizer.pad_token_id:\n",
    "            w.append(not_carry_w)\n",
    "        else: break\n",
    "    w += [0.0] * (input_size+1 - len(w))\n",
    "    weights.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-20T16:07:09.036506Z",
     "iopub.status.idle": "2025-02-20T16:07:09.037038Z",
     "shell.execute_reply": "2025-02-20T16:07:09.036811Z",
     "shell.execute_reply.started": "2025-02-20T16:07:09.036789Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X = tf.constant(train_padded, shape=(len(train_padded), input_size+1), dtype=tf.int32)\n",
    "T = tf.constant(test_padded,  shape=(len(test_padded),  input_size+1), dtype=tf.int32)\n",
    "W = tf.constant(weights,      shape=(len(weights),      input_size+1), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model\n",
    "Defining the embedding layer, differential attention layer and transformer model architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T16:39:45.740645Z",
     "iopub.status.busy": "2025-03-01T16:39:45.740193Z",
     "iopub.status.idle": "2025-03-01T16:39:45.748886Z",
     "shell.execute_reply": "2025-03-01T16:39:45.747419Z",
     "shell.execute_reply.started": "2025-03-01T16:39:45.740623Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def masked_accuracy(y_true, y_pred, padding_token=tokenizer.pad_token_id):\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "    y_pred = tf.cast(tf.argmax(y_pred, axis=-1), tf.int32)\n",
    "    mask = tf.cast(tf.not_equal(y_true, padding_token), tf.float32)\n",
    "    matches = tf.cast(tf.equal(y_true, y_pred), tf.float32)\n",
    "    accuracy = tf.reduce_sum(matches * mask) / tf.reduce_sum(mask)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T16:39:45.750946Z",
     "iopub.status.busy": "2025-03-01T16:39:45.750579Z",
     "iopub.status.idle": "2025-03-01T16:39:45.774773Z",
     "shell.execute_reply": "2025-03-01T16:39:45.772339Z",
     "shell.execute_reply.started": "2025-03-01T16:39:45.750918Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SharedEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embed_dim, **kwargs):\n",
    "        super(SharedEmbedding, self).__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.shared_weights = self.add_weight(\n",
    "            shape=(self.vocab_size, self.embed_dim),\n",
    "            initializer='random_normal',\n",
    "            trainable=True,\n",
    "            name='shared_weights'\n",
    "        )\n",
    "        super(SharedEmbedding, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs, mode='embedding'):\n",
    "        if mode == 'embedding':\n",
    "            return tf.nn.embedding_lookup(self.shared_weights, inputs)\n",
    "        elif mode == 'classify':\n",
    "            return tf.nn.softmax(tf.matmul(inputs, self.shared_weights, transpose_b=True), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T18:30:03.936125Z",
     "iopub.status.busy": "2025-03-01T18:30:03.935118Z",
     "iopub.status.idle": "2025-03-01T18:30:03.962479Z",
     "shell.execute_reply": "2025-03-01T18:30:03.961436Z",
     "shell.execute_reply.started": "2025-03-01T18:30:03.936090Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DiffAttention(keras.layers.Layer):\n",
    "    def __init__(self, depth, **kwargs):\n",
    "        super(DiffAttention, self).__init__(**kwargs)\n",
    "        self.lambda_init = 0.8 - 0.6 * math.exp(-0.3 * depth)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.embed_dim = input_shape[-1]\n",
    "        self.input_size = input_shape[-2]\n",
    "        self.mask = tf.where(tf.linalg.band_part(tf.ones((input_shape[-2], input_shape[-2])), -1, 0) == 1.0, 0.0, float(\"-inf\"))\n",
    "        self.range_do = -tf.range(input_shape[-2])-1\n",
    "        self.range_undo = tf.range(input_shape[-2])+1\n",
    "        self.Q = self.add_weight(name='kernelQ',\n",
    "                                      shape=(input_shape[-1], input_shape[-1]),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        self.K = self.add_weight(name='kernelK',\n",
    "                                      shape=(input_shape[-1], input_shape[-1]),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        self.V = self.add_weight(name='kernelV',\n",
    "                                      shape=(input_shape[-1], input_shape[-1]),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "\n",
    "        initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.1)\n",
    "        self.lambda_q1 = self.add_weight(\n",
    "            shape=(input_shape[-1],), initializer=initializer, trainable=True, name=\"lambda_q1\"\n",
    "        )\n",
    "        self.lambda_k1 = self.add_weight(\n",
    "            shape=(input_shape[-1],), initializer=initializer, trainable=True, name=\"lambda_k1\"\n",
    "        )\n",
    "        self.lambda_q2 = self.add_weight(\n",
    "            shape=(input_shape[-1],), initializer=initializer, trainable=True, name=\"lambda_q2\"\n",
    "        )\n",
    "        self.lambda_k2 = self.add_weight(\n",
    "            shape=(input_shape[-1],), initializer=initializer, trainable=True, name=\"lambda_k2\"\n",
    "        )\n",
    "        \n",
    "        super(DiffAttention, self).build(input_shape)\n",
    "\n",
    "    def roll_embeddings(self, tensor, shift_values):\n",
    "        batch_size, time_size, embed_dim = tensor.shape\n",
    "        if batch_size is None: return tensor\n",
    "        shift_matrix   = tf.reshape(shift_values, (1, -1, 1))\n",
    "        shift_matrix   = tf.tile(shift_matrix, [batch_size, 1, embed_dim])\n",
    "        indices        = tf.range(embed_dim)\n",
    "        indices_matrix = tf.tile(indices, [batch_size * time_size])\n",
    "        indices_matrix = tf.reshape(indices_matrix, (batch_size, time_size, embed_dim))\n",
    "        new_indices    = (indices_matrix + shift_matrix) % embed_dim     \n",
    "        rolled_tensor  = tf.gather(tensor, new_indices, batch_dims=2)\n",
    "        return rolled_tensor\n",
    "\n",
    "    def call(self, x, pos):\n",
    "        v    = x @ self.V\n",
    "        q    = tf.transpose(tf.reshape(x @ self.Q, (-1, self.input_size, 2, self.embed_dim//2)), perm=[0, 2, 1, 3])\n",
    "        k    = tf.transpose(tf.reshape(x @ self.K, (-1, self.input_size, 2, self.embed_dim//2)), perm=[0, 2, 1, 3])\n",
    "        atti = tf.matmul(q, k,   transpose_b=True)\n",
    "        attp = tf.matmul(q, pos, transpose_b=True)\n",
    "        attp = self.roll_embeddings(tf.reshape(attp, (-1, self.input_size, self.input_size)), self.range_do)\n",
    "        attp = tf.reshape(attp, (-1, 2, self.input_size, self.input_size))\n",
    "        att  = atti + attp\n",
    "        att  = tf.nn.softmax((att / math.sqrt(self.embed_dim)) + self.mask, axis=-1)\n",
    "        att1 = att[:, 0]\n",
    "        att2 = att[:, 1]\n",
    "        \n",
    "        lambda_1 = tf.math.exp(tf.reduce_sum(self.lambda_q1 * self.lambda_k1, axis=-1))\n",
    "        lambda_2 = tf.math.exp(tf.reduce_sum(self.lambda_q2 * self.lambda_k2, axis=-1))\n",
    "        lambda_full = lambda_1 - lambda_2 + self.lambda_init\n",
    "        att = att1 - lambda_full * att2\n",
    "\n",
    "        out = att @ v\n",
    "        out = out * (1 - self.lambda_init)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-20T17:16:56.464863Z",
     "iopub.status.busy": "2025-02-20T17:16:56.464301Z",
     "iopub.status.idle": "2025-02-20T17:16:59.161329Z",
     "shell.execute_reply": "2025-02-20T17:16:59.160222Z",
     "shell.execute_reply.started": "2025-02-20T17:16:56.464822Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(input_size, ), dtype=tf.int32)\n",
    "emb_layer = SharedEmbedding(vocab_size, embed_dim)\n",
    "pos_layer = keras_nlp.layers.PositionEmbedding(input_size)\n",
    "\n",
    "ins = LayerNormalization()(emb_layer(inputs, mode=\"embedding\"))\n",
    "x = ins\n",
    "pos_src = pos_layer(x)\n",
    "pos = tf.transpose(tf.reshape(pos_src, (-1, input_size, 2, embed_dim//2)), perm=[0, 2, 1, 3])\n",
    "\n",
    "b = 12\n",
    "for d in range(b):\n",
    "    x += (2*b)**-0.5 * LayerNormalization()(DiffAttention(d+1)(x, pos))\n",
    "    x1 = Dense(embed_dim, activation=\"gelu\")(x)\n",
    "    x1 = Dense(embed_dim, activation=\"gelu\")(x1)\n",
    "    x += (2*b)**-0.5 * LayerNormalization()(x1)\n",
    "\n",
    "x = emb_layer(x, mode=\"classify\")\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=x)\n",
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(ignore_class=tokenizer.pad_token_id),\n",
    "    optimizer=keras.optimizers.AdamW(learning_rate=0.001),\n",
    "    metrics=[masked_accuracy, keras_nlp.metrics.Perplexity(mask_token_id=tokenizer.pad_token_id)],\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-18T17:31:47.704Z",
     "iopub.status.busy": "2025-02-18T17:22:34.715840Z",
     "iopub.status.idle": "2025-02-18T17:22:34.716141Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for i in range(30):\n",
    "    \n",
    "    if i % 2 == 1 and i < 20:\n",
    "        w = tf.where(W < 1.0, 0.05, 1.0) \n",
    "    else:\n",
    "        w = tf.where(W < 1.0, 1.0, 1.0)\n",
    "    \n",
    "    model.fit(\n",
    "        x=X[:, :-1],\n",
    "        y=X[:, 1:],\n",
    "        shuffle=True,\n",
    "        epochs=1,\n",
    "        batch_size=60,\n",
    "        sample_weight=w[:, 1:]\n",
    "    )\n",
    "    \n",
    "    model.save(\"rpc.keras\")\n",
    "    \n",
    "    print(f\"Epoch {i+1} completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model\n",
    "Loading the model from file and creating helper function to vectorize texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T18:30:13.899505Z",
     "iopub.status.busy": "2025-03-01T18:30:13.899163Z",
     "iopub.status.idle": "2025-03-01T18:30:23.881753Z",
     "shell.execute_reply": "2025-03-01T18:30:23.880020Z",
     "shell.execute_reply.started": "2025-03-01T18:30:13.899484Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\n",
    "    \"rpc.keras\",\n",
    "    custom_objects={\n",
    "        \"DiffAttention\" : DiffAttention,\n",
    "        \"SharedEmbedding\" : SharedEmbedding,\n",
    "        \"masked_accuracy\" : masked_accuracy\n",
    "    }\n",
    ")\n",
    "encoder = keras.Model(inputs=model.layers[0].input, outputs=model.layers[-1].output)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T18:30:23.889245Z",
     "iopub.status.busy": "2025-03-01T18:30:23.888858Z",
     "iopub.status.idle": "2025-03-01T18:30:25.795027Z",
     "shell.execute_reply": "2025-03-01T18:30:25.794212Z",
     "shell.execute_reply.started": "2025-03-01T18:30:23.889202Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def vectorize_texts(all_texts):\n",
    "    batch_size = 128\n",
    "    vects = []\n",
    "    for i in range(0, len(all_texts), batch_size):\n",
    "        texts = all_texts[i:i+batch_size]\n",
    "        toks = [text + ([tokenizer.pad_token_id] * (input_size - len(text))) for text in texts]\n",
    "        if len(toks) > 0:\n",
    "            toks = tf.constant(toks, shape=(len(toks), input_size))\n",
    "            vect = encoder.predict(toks, verbose=0)\n",
    "            for v, t in zip(vect, texts):\n",
    "                vects.append(v[:len(t), :])\n",
    "    return tf.concat(vects, axis=0).numpy()\n",
    "\n",
    "vectorize_texts([\n",
    "    tokenizer.encode(\"<s>Hello there. how are you?\", add_special_tokens=False),\n",
    "    tokenizer.encode(\"<s>Hello there. how have you been?\", add_special_tokens=False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NGT Based Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T14:16:25.932211Z",
     "iopub.status.busy": "2025-02-28T14:16:25.931763Z",
     "iopub.status.idle": "2025-02-28T14:19:56.339029Z",
     "shell.execute_reply": "2025-02-28T14:19:56.337419Z",
     "shell.execute_reply.started": "2025-02-28T14:16:25.932180Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/jpmag7/NGT.git\n",
    "%cd NGT\n",
    "!mkdir build\n",
    "%cd build\n",
    "!cmake -DNGT_SHARED_MEMORY_ALLOCATOR=ON ..\n",
    "!make\n",
    "!make install\n",
    "!ldconfig /usr/local/lib\n",
    "%cd /kaggle/working/NGT/python\n",
    "!python3 setup.py sdist\n",
    "!pip3 install dist/ngt-2.2.4.tar.gz\n",
    "%cd /kaggle/working\n",
    "!rm -r NGT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T14:19:56.341947Z",
     "iopub.status.busy": "2025-02-28T14:19:56.341567Z",
     "iopub.status.idle": "2025-02-28T15:05:36.026365Z",
     "shell.execute_reply": "2025-02-28T15:05:36.024705Z",
     "shell.execute_reply.started": "2025-02-28T14:19:56.341911Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import ngtpy\n",
    "import json\n",
    "\n",
    "size = 30_000\n",
    "batch_size = 2048\n",
    "\n",
    "index_path = \"index\"\n",
    "ngtpy.create(index_path, embed_dim)\n",
    "index = ngtpy.Index(index_path)\n",
    "\n",
    "all_toks = []\n",
    "\n",
    "for start in tqdm(range(0, size, batch_size)):\n",
    "    \n",
    "    prompt_embeds = vectorize_texts([t[:-1] for t in train[start:min(size, start+batch_size)]])\n",
    "    \n",
    "    chars = [t for text in train[start:min(size, start+batch_size)] for t in text[1:]]\n",
    "    for c in chars: all_toks.append(c)\n",
    "\n",
    "    if prompt_embeds.shape[0] > 0: index.batch_insert(prompt_embeds)\n",
    "    \n",
    "with open(\"index/all_toks.json\", \"w\") as f:\n",
    "    f.write(json.dumps(all_toks))\n",
    "\n",
    "print(\"building objects...\")\n",
    "index.build_index()\n",
    "print(\"saving the index...\")\n",
    "index.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T11:01:02.803163Z",
     "iopub.status.busy": "2025-02-25T11:01:02.802609Z",
     "iopub.status.idle": "2025-02-25T11:01:03.597044Z",
     "shell.execute_reply": "2025-02-25T11:01:03.595800Z",
     "shell.execute_reply.started": "2025-02-25T11:01:02.803120Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "index_path = \"/kaggle/working/index\"\n",
    "index = ngtpy.Index(index_path, read_only=True)\n",
    "\n",
    "with open(\"all_toks.json\", \"r\") as f:\n",
    "    all_toks = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flat Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T19:11:54.499934Z",
     "iopub.status.busy": "2025-03-01T19:11:54.499554Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "size = 30_000\n",
    "\n",
    "all_toks = [t for text in train[:size] for t in text[1:]]\n",
    "with open(\"all_toks.json\", \"w\") as f:\n",
    "    f.write(json.dumps(all_toks))\n",
    "\n",
    "embeds = []\n",
    "batch_size = 2048\n",
    "for start in tqdm(range(0, size, batch_size)):\n",
    "    prompt_embeds = vectorize_texts([t[:-1] for t in train[start:min(size, start+batch_size)]])\n",
    "    embeds.append(prompt_embeds)\n",
    "embeds = tf.concat(embeds, axis=0)\n",
    "\n",
    "import faiss\n",
    "index = faiss.IndexFlatL2(embed_dim)\n",
    "index.add(embeds)\n",
    "faiss.write_index(index, \"index.faiss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T19:10:54.343662Z",
     "iopub.status.busy": "2025-03-01T19:10:54.343320Z",
     "iopub.status.idle": "2025-03-01T19:11:37.482149Z",
     "shell.execute_reply": "2025-03-01T19:11:37.480819Z",
     "shell.execute_reply.started": "2025-03-01T19:10:54.343640Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "enc_text = tokenizer.encode(\"<s>\", add_special_tokens=False)\n",
    "sents = []\n",
    "while True:\n",
    "    user = input(f\"{len(enc_text)}>\") + \"\\n\"\n",
    "    user = tokenizer.encode(user, add_special_tokens=False)\n",
    "    sents.append(user)\n",
    "    enc_text += user\n",
    "    new_text = tokenizer.decode(enc_text)\n",
    "    text = new_text\n",
    "    tok = 0\n",
    "    sents.append([])\n",
    "    while tok != vocab_size - 2:\n",
    "        xq = vectorize_texts([enc_text])[-1]\n",
    "\n",
    "        # If using faiss index\n",
    "        _id = index.search(xq.reshape((1, -1)), 1)[1][0][0]\n",
    "        \n",
    "        # If using ngt index\n",
    "        #_id = index.search(xq, size=1, epsilon=1)[0][0]\n",
    "        \n",
    "        if all_toks[_id] in carry_toks:\n",
    "            tmp = tf.argmax(tf.matmul(xq.reshape((1, -1)), encoder.layers[1].shared_weights, transpose_b=True), axis=-1).numpy()[0]\n",
    "            if tmp in enc_text: tok = tmp\n",
    "            else: tok = all_toks[_id]\n",
    "        else:\n",
    "            tok = all_toks[_id]\n",
    "\n",
    "        sents[-1].append(tok)\n",
    "        enc_text += [tok]\n",
    "        new_text = tokenizer.decode(enc_text)\n",
    "        print(new_text[len(text):], end=\"\")\n",
    "        text = new_text\n",
    "    print(\"\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11258714,
     "datasetId": 4261095,
     "sourceId": 10890851,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 11251702,
     "datasetId": 5560278,
     "sourceId": 10884533,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
