{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b42dc02c-ab81-4fd0-89bc-782481558a2e","_uuid":"e94b73d6-c4ce-416c-949c-37c3a3df77e7","collapsed":false,"execution":{"iopub.execute_input":"2024-07-29T13:00:06.696236Z","iopub.status.busy":"2024-07-29T13:00:06.695845Z","iopub.status.idle":"2024-07-29T13:00:45.301376Z","shell.execute_reply":"2024-07-29T13:00:45.300119Z","shell.execute_reply.started":"2024-07-29T13:00:06.696197Z"},"jupyter":{"outputs_hidden":false},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting keras-nlp==0.10.0\n","  Downloading keras_nlp-0.10.0-py3-none-any.whl.metadata (7.0 kB)\n","Collecting keras-core (from keras-nlp==0.10.0)\n","  Downloading keras_core-0.1.7-py3-none-any.whl.metadata (4.3 kB)\n","Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from keras-nlp==0.10.0) (1.4.0)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from keras-nlp==0.10.0) (1.26.4)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from keras-nlp==0.10.0) (21.3)\n","Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from keras-nlp==0.10.0) (2023.12.25)\n","Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras-nlp==0.10.0) (13.7.0)\n","Requirement already satisfied: dm-tree in /opt/conda/lib/python3.10/site-packages (from keras-nlp==0.10.0) (0.1.8)\n","Requirement already satisfied: kagglehub in /opt/conda/lib/python3.10/site-packages (from keras-nlp==0.10.0) (0.2.3)\n","Requirement already satisfied: tensorflow-text in /opt/conda/lib/python3.10/site-packages (from keras-nlp==0.10.0) (2.15.0)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from kagglehub->keras-nlp==0.10.0) (2.31.0)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from kagglehub->keras-nlp==0.10.0) (4.66.1)\n","Requirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras-core->keras-nlp==0.10.0) (0.0.8)\n","Requirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from keras-core->keras-nlp==0.10.0) (3.10.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->keras-nlp==0.10.0) (3.1.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras-nlp==0.10.0) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras-nlp==0.10.0) (2.17.2)\n","Requirement already satisfied: tensorflow-hub>=0.13.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-text->keras-nlp==0.10.0) (0.16.1)\n","Requirement already satisfied: tensorflow<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-text->keras-nlp==0.10.0) (2.15.0)\n","Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras-nlp==0.10.0) (0.1.2)\n","Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.10.0) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.10.0) (23.5.26)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.10.0) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.10.0) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.10.0) (16.0.6)\n","Requirement already satisfied: ml-dtypes~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.10.0) (0.2.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.10.0) (3.3.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.10.0) (3.20.3)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.10.0) (69.0.3)\n","Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.10.0) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.10.0) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.10.0) (4.9.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.10.0) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.10.0) (0.35.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.10.0) (1.60.0)\n","Requirement already satisfied: tensorboard<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.10.0) (2.15.1)\n","Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.10.0) (2.15.0)\n","Collecting keras<2.16,>=2.15.0 (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.10.0)\n","  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n","Requirement already satisfied: tf-keras>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow-hub>=0.13.0->tensorflow-text->keras-nlp==0.10.0) (2.15.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->kagglehub->keras-nlp==0.10.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->kagglehub->keras-nlp==0.10.0) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->kagglehub->keras-nlp==0.10.0) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->kagglehub->keras-nlp==0.10.0) (2024.2.2)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.10.0) (0.42.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.10.0) (2.26.1)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.10.0) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.10.0) (3.5.2)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.10.0) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.10.0) (3.0.2)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.10.0) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.10.0) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.10.0) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.10.0) (1.3.1)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.10.0) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.10.0) (0.5.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.10.0) (3.2.2)\n","Downloading keras_nlp-0.10.0-py3-none-any.whl (513 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m513.7/513.7 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading keras_core-0.1.7-py3-none-any.whl (950 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: keras, keras-core, keras-nlp\n","  Attempting uninstall: keras\n","    Found existing installation: keras 3.2.1\n","    Uninstalling keras-3.2.1:\n","      Successfully uninstalled keras-3.2.1\n","  Attempting uninstall: keras-nlp\n","    Found existing installation: keras-nlp 0.9.3\n","    Uninstalling keras-nlp-0.9.3:\n","      Successfully uninstalled keras-nlp-0.9.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed keras-2.15.0 keras-core-0.1.7 keras-nlp-0.10.0\n","Collecting faiss-cpu\n","  Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n","Requirement already satisfied: numpy<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from faiss-cpu) (1.26.4)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from faiss-cpu) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->faiss-cpu) (3.1.1)\n","Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: faiss-cpu\n","Successfully installed faiss-cpu-1.8.0.post1\n"]}],"source":["!pip install keras-nlp==0.10.0\n","!pip install faiss-cpu\n","!pip install keras==2.15.0\n","!pip install tensorflow==2.15.0"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"f40f8265-dd13-4005-9b98-0e8497ebd05c","_uuid":"2d66e846-b199-412a-9ca2-1e638ebef526","collapsed":false,"execution":{"iopub.execute_input":"2024-07-29T13:00:45.303843Z","iopub.status.busy":"2024-07-29T13:00:45.303474Z","iopub.status.idle":"2024-07-29T13:01:02.251170Z","shell.execute_reply":"2024-07-29T13:01:02.250216Z","shell.execute_reply.started":"2024-07-29T13:00:45.303809Z"},"jupyter":{"outputs_hidden":false},"scrolled":true,"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from keras.layers import *\n","import keras_nlp\n","\n","import math\n","import spacy\n","import numpy as np\n","import random\n","import json"]},{"cell_type":"code","execution_count":5,"metadata":{"_cell_guid":"5408a093-8dc1-4d25-bc4a-baa07b73f1aa","_uuid":"f2cf7ba5-811c-4dd3-8884-a5ec196c9707","collapsed":false,"execution":{"iopub.execute_input":"2024-07-29T13:01:02.261302Z","iopub.status.busy":"2024-07-29T13:01:02.260890Z","iopub.status.idle":"2024-07-29T13:01:08.793271Z","shell.execute_reply":"2024-07-29T13:01:08.792114Z","shell.execute_reply.started":"2024-07-29T13:01:02.261266Z"},"jupyter":{"outputs_hidden":false},"scrolled":true,"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"]},{"name":"stdout","output_type":"stream","text":["vocab_size: 32102\n","interrogation id: 3, newline id: 32100, column id: 10\n","[32101, 8774, 21820, 6, 149, 33, 25, 469, 58, 207, 6, 131, 1705, 14145, 8585, 7, 233, 32100]\n","32101 <s>\n","8774 Hello\n","21820 hello\n","6 ,\n","149 how\n","33 are\n","25 you\n","469 today\n","58 ?\n","207 good\n","6 ,\n","131 just\n","1705 understanding\n","14145 token\n","8585 izer\n","7 s\n","233 ...\n","32100 \n","\n","<s> Hello hello, how are you today? good, just understanding tokenizers...\n","\n","<pad>\n"]}],"source":["from transformers import AutoTokenizer\n","from tokenizers import AddedToken\n","\n","tokenizer = AutoTokenizer.from_pretrained('google/t5-v1_1-base')\n","tokenizer.add_tokens(AddedToken(\"\\n\", normalized=False))\n","tokenizer.add_tokens(AddedToken(\"<s>\", normalized=False))\n","vocab_size = len(tokenizer.get_vocab().keys())\n","print(\"vocab_size:\", vocab_size)\n","\n","interrogation_id = tokenizer.encode(\"?\", add_special_tokens=False)[0]\n","newline_id = tokenizer.encode(\"\\n\", add_special_tokens=False)[0]\n","column_id = tokenizer.encode(\":\", add_special_tokens=False)[-1]\n","print(f\"interrogation id: {interrogation_id}, newline id: {newline_id}, column id: {column_id}\")\n","\n","text = \"<s>Hello hello, how are you today? good, just understanding tokenizers...\\n\"\n","tokens = tokenizer.encode(text, add_special_tokens=False)\n","print(tokens)\n","for t in tokens: print(t, tokenizer.decode([t]))\n","text = tokenizer.decode(tokens, skip_special_tokens=True)\n","print(text)\n","print(tokenizer.pad_token)"]},{"cell_type":"code","execution_count":8,"metadata":{"_cell_guid":"62597560-9627-41e4-a0a2-130a5862e84f","_uuid":"c5b9dc33-f611-4727-9430-74be38173eb6","collapsed":false,"execution":{"iopub.execute_input":"2024-07-29T13:01:08.841517Z","iopub.status.busy":"2024-07-29T13:01:08.841083Z","iopub.status.idle":"2024-07-29T13:04:33.321016Z","shell.execute_reply":"2024-07-29T13:04:33.319775Z","shell.execute_reply.started":"2024-07-29T13:01:08.841481Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (789 > 512). Running this sequence through the model will result in indexing errors\n"]},{"data":{"text/plain":["0"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["DATASET_PATH = \"dataset.json\"\n","\n","file = open(DATASET_PATH, \"r\")\n","dataset = json.loads(file.read())\n","file.close()\n","\n","data = []\n","for text in dataset:\n","    text = \"\".join(text)\n","    text = tokenizer.encode(\"<s>\" + text, add_special_tokens=False)\n","    data.append(text)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-20T00:17:17.070461Z","iopub.status.busy":"2024-07-20T00:17:17.070057Z","iopub.status.idle":"2024-07-20T00:17:39.446550Z","shell.execute_reply":"2024-07-20T00:17:39.445013Z","shell.execute_reply.started":"2024-07-20T00:17:17.070430Z"},"trusted":true},"outputs":[],"source":["nlp = spacy.load(\"en_core_web_lg\")\n","nlp.max_length = 2000000\n","\n","# 'PART', 'INTJ', 'SPACE', 'AUX', 'PUNCT', 'SYM', 'X', 'SCONJ', 'NUM', 'NOUN', 'ADP', 'ADJ', 'ADV', 'PRON', 'DET', 'CCONJ', 'PROPN', 'VERB'\n","selected = {'NUM', 'NOUN', 'ADJ', 'ADV', 'PROPN'}\n","\n","all_toks = sorted(list(tokenizer.get_vocab().items()), key=lambda x:x[1])\n","all_toks_text = \"\\n\".join([t[0].replace(\"▁\", \"\") for t in all_toks])\n","\n","doc = nlp(all_toks_text)\n","\n","carry_toks = set()\n","\n","print(len(doc), len(all_toks))\n","\n","i = 0\n","for ii, token in enumerate(doc):\n","    if str(token) in all_toks[i][0]: pass\n","    else: i += 1\n","    if str(token) in all_toks[i][0] and token.pos_ in selected and i > 100:\n","        carry_toks.add(all_toks[i][1])\n","print(len(carry_toks))"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d8902183-0b87-4185-910d-46d2bfb7b2f5","_uuid":"29ab352a-86e4-4b60-a5a3-917a77582a86","collapsed":false,"execution":{"iopub.execute_input":"2024-07-20T00:17:39.449477Z","iopub.status.busy":"2024-07-20T00:17:39.448747Z","iopub.status.idle":"2024-07-20T00:17:39.482851Z","shell.execute_reply":"2024-07-20T00:17:39.481421Z","shell.execute_reply.started":"2024-07-20T00:17:39.449442Z"},"jupyter":{"outputs_hidden":false},"scrolled":true,"trusted":true},"outputs":[],"source":["data_size = len(data)\n","\n","def get_random_sample(input_size):\n","    rnd  = random.randint(0, data_size-1)\n","    text = data[rnd]\n","    pos  = random.randint(min(input_size, len(text)-1), len(text)-1)\n","    text = text[:pos]\n","    x    = text[-input_size:]\n","    \n","    in_past = set()\n","    weights = []\n","    for t in x:\n","        if t in carry_toks:\n","            if t in in_past:\n","                weights.append(1.0)\n","            else:\n","                in_past.add(t)\n","                weights.append(0.6)\n","        elif t != tokenizer.pad_token_id:\n","            weights.append(0.6)\n","        else: break\n","    x = x + [tokenizer.pad_token_id] * (input_size - len(x))\n","    weights = weights + [0.0] * (input_size - len(weights))\n","    return x, weights\n","\n","x, w = get_random_sample(256)\n","print(tokenizer.decode(x[:-1]), \"\\n>\", tokenizer.decode(x[1:]))"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"72475573-1f9c-40f0-a3bf-d7c3eafecc1f","_uuid":"d4413b24-015d-42a5-beb0-f4ddbd11f21d","collapsed":false,"execution":{"iopub.execute_input":"2024-07-20T00:17:39.484813Z","iopub.status.busy":"2024-07-20T00:17:39.484405Z","iopub.status.idle":"2024-07-20T00:17:39.499700Z","shell.execute_reply":"2024-07-20T00:17:39.498221Z","shell.execute_reply.started":"2024-07-20T00:17:39.484772Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["def get_train_batch(batch_size, input_size):\n","    X = []\n","    W = []\n","    for _ in range(batch_size):\n","        x, w = get_random_sample(input_size)\n","        X.append(x)\n","        W.append(w)\n","    X = tf.constant(X, shape=(len(X), input_size), dtype=tf.int32)\n","    W = tf.constant(W, shape=(len(W), input_size), dtype=tf.float32)\n","    return X, W"]},{"cell_type":"code","execution_count":6,"metadata":{"_cell_guid":"32ed2cbd-b388-41be-9ba1-55746098c1f7","_uuid":"a087ac59-0689-45c1-b6c9-33d8369c3b65","collapsed":false,"execution":{"iopub.execute_input":"2024-07-29T13:04:33.332856Z","iopub.status.busy":"2024-07-29T13:04:33.332440Z","iopub.status.idle":"2024-07-29T13:04:33.351931Z","shell.execute_reply":"2024-07-29T13:04:33.350693Z","shell.execute_reply.started":"2024-07-29T13:04:33.332825Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["def roll_embeddings(tensor, shift_values):\n","    batch_size, time_size, embed_size = tensor.shape\n","    if batch_size is None: return tensor\n","\n","    shift_matrix = tf.reshape(shift_values, (1, -1, 1))\n","    shift_matrix = tf.tile(shift_matrix, [batch_size, 1, embed_size])\n","    \n","    indices = tf.range(embed_size)\n","    indices_matrix = tf.tile(indices, [batch_size * time_size])\n","    indices_matrix = tf.reshape(indices_matrix, (batch_size, time_size, embed_size))\n","    \n","    new_indices = (indices_matrix + shift_matrix) % embed_size\n","    \n","    rolled_tensor = tf.gather(tensor, new_indices, batch_dims=2)\n","    \n","    return rolled_tensor"]},{"cell_type":"code","execution_count":7,"metadata":{"_cell_guid":"12324a6d-1ec9-4399-954c-cd5be19cb966","_uuid":"980ca45f-b567-46b0-9627-bb79e7d4f3ff","collapsed":false,"execution":{"iopub.execute_input":"2024-07-29T13:04:33.353583Z","iopub.status.busy":"2024-07-29T13:04:33.353238Z","iopub.status.idle":"2024-07-29T13:04:33.372780Z","shell.execute_reply":"2024-07-29T13:04:33.371677Z","shell.execute_reply.started":"2024-07-29T13:04:33.353555Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["class Attention(keras.layers.Layer):\n","    def __init__(self, **kwargs):\n","        super(Attention, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        self.embed_size = input_shape[-1]\n","        self.mask = tf.where(tf.linalg.band_part(tf.ones((input_shape[-2], input_shape[-2])), -1, 0) == 1.0, 0.0, float(\"-inf\"))\n","        self.range_do = -tf.range(input_shape[-2])-1\n","        self.range_undo = tf.range(input_shape[-2])+1\n","        self.Q = self.add_weight(name='kernelQ',\n","                                      shape=(input_shape[-1], input_shape[-1]),\n","                                      initializer='uniform',\n","                                      trainable=True)\n","        self.K = self.add_weight(name='kernelK',\n","                                      shape=(input_shape[-1], input_shape[-1]),\n","                                      initializer='uniform',\n","                                      trainable=True)\n","        self.V = self.add_weight(name='kernelV',\n","                                      shape=(input_shape[-1], input_shape[-1]),\n","                                      initializer='uniform',\n","                                      trainable=True)\n","        super(Attention, self).build(input_shape)\n","\n","    def call(self, x, pos, ret=False):\n","        q    = x @ self.Q\n","        k    = x @ self.K\n","        v    = x @ self.V\n","        atti = tf.matmul(q, k,   transpose_b=True)\n","        attp = tf.matmul(q, pos, transpose_b=True)\n","        attp = roll_embeddings(attp, self.range_do)\n","        att  = atti + attp\n","        att  = tf.nn.softmax((att / math.sqrt(self.embed_size)) + self.mask, axis=-1)\n","        outi = att @ v\n","        attp = roll_embeddings(att, self.range_undo)\n","        outp = attp @ pos\n","        out  = outi + outp\n","        return out"]},{"cell_type":"code","execution_count":8,"metadata":{"_cell_guid":"061315bd-b3f4-41e2-9edf-a2d43e1258ca","_uuid":"40418cf9-3995-4ca2-b619-d8c8f8c37d1e","collapsed":false,"execution":{"iopub.execute_input":"2024-07-29T13:04:33.374699Z","iopub.status.busy":"2024-07-29T13:04:33.374267Z","iopub.status.idle":"2024-07-29T13:04:33.390319Z","shell.execute_reply":"2024-07-29T13:04:33.389099Z","shell.execute_reply.started":"2024-07-29T13:04:33.374660Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["def masked_accuracy(y_true, y_pred, padding_token=tokenizer.pad_token_id):\n","    y_true = tf.cast(y_true, tf.int32)\n","    y_pred = tf.cast(tf.argmax(y_pred, axis=-1), tf.int32)\n","\n","    mask = tf.cast(tf.not_equal(y_true, padding_token), tf.float32)\n","    matches = tf.cast(tf.equal(y_true, y_pred), tf.float32)\n","    \n","    accuracy = tf.reduce_sum(matches * mask) / tf.reduce_sum(mask)\n","    return accuracy"]},{"cell_type":"code","execution_count":9,"metadata":{"_cell_guid":"5ecf2d11-ab90-4f14-94fa-a9ec89108e47","_uuid":"e0a95c1e-b00c-4a1f-88d5-c3d08c4cc824","collapsed":false,"execution":{"iopub.execute_input":"2024-07-29T13:04:33.392916Z","iopub.status.busy":"2024-07-29T13:04:33.391851Z","iopub.status.idle":"2024-07-29T13:04:35.803713Z","shell.execute_reply":"2024-07-29T13:04:35.802640Z","shell.execute_reply.started":"2024-07-29T13:04:33.392883Z"},"jupyter":{"outputs_hidden":false},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (lambda), but\n","are not present in its tracked objects:\n","  <tf.Variable 'embedding/embeddings:0' shape=(32103, 128) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_1 (InputLayer)        [(None, 512)]                0         []                            \n","                                                                                                  \n"," embedding (Embedding)       (None, 512, 128)             4109184   ['input_1[0][0]']             \n","                                                                                                  \n"," layer_normalization (Layer  (None, 512, 128)             256       ['embedding[0][0]']           \n"," Normalization)                                                                                   \n","                                                                                                  \n"," position_embedding (Positi  (None, 512, 128)             65536     ['layer_normalization[0][0]'] \n"," onEmbedding)                                                                                     \n","                                                                                                  \n"," attention (Attention)       (None, 512, 128)             49152     ['layer_normalization[0][0]', \n","                                                                     'position_embedding[0][0]']  \n","                                                                                                  \n"," layer_normalization_1 (Lay  (None, 512, 128)             256       ['attention[0][0]']           \n"," erNormalization)                                                                                 \n","                                                                                                  \n"," tf.math.multiply (TFOpLamb  (None, 512, 128)             0         ['layer_normalization_1[0][0]'\n"," da)                                                                ]                             \n","                                                                                                  \n"," tf.__operators__.add (TFOp  (None, 512, 128)             0         ['layer_normalization[0][0]', \n"," Lambda)                                                             'tf.math.multiply[0][0]']    \n","                                                                                                  \n"," attention_1 (Attention)     (None, 512, 128)             49152     ['tf.__operators__.add[0][0]',\n","                                                                     'position_embedding[0][0]']  \n","                                                                                                  \n"," layer_normalization_2 (Lay  (None, 512, 128)             256       ['attention_1[0][0]']         \n"," erNormalization)                                                                                 \n","                                                                                                  \n"," tf.math.multiply_1 (TFOpLa  (None, 512, 128)             0         ['layer_normalization_2[0][0]'\n"," mbda)                                                              ]                             \n","                                                                                                  \n"," tf.__operators__.add_1 (TF  (None, 512, 128)             0         ['tf.__operators__.add[0][0]',\n"," OpLambda)                                                           'tf.math.multiply_1[0][0]']  \n","                                                                                                  \n"," attention_2 (Attention)     (None, 512, 128)             49152     ['tf.__operators__.add_1[0][0]\n","                                                                    ',                            \n","                                                                     'position_embedding[0][0]']  \n","                                                                                                  \n"," layer_normalization_3 (Lay  (None, 512, 128)             256       ['attention_2[0][0]']         \n"," erNormalization)                                                                                 \n","                                                                                                  \n"," tf.math.multiply_2 (TFOpLa  (None, 512, 128)             0         ['layer_normalization_3[0][0]'\n"," mbda)                                                              ]                             \n","                                                                                                  \n"," tf.__operators__.add_2 (TF  (None, 512, 128)             0         ['tf.__operators__.add_1[0][0]\n"," OpLambda)                                                          ',                            \n","                                                                     'tf.math.multiply_2[0][0]']  \n","                                                                                                  \n"," attention_3 (Attention)     (None, 512, 128)             49152     ['tf.__operators__.add_2[0][0]\n","                                                                    ',                            \n","                                                                     'position_embedding[0][0]']  \n","                                                                                                  \n"," layer_normalization_4 (Lay  (None, 512, 128)             256       ['attention_3[0][0]']         \n"," erNormalization)                                                                                 \n","                                                                                                  \n"," tf.math.multiply_3 (TFOpLa  (None, 512, 128)             0         ['layer_normalization_4[0][0]'\n"," mbda)                                                              ]                             \n","                                                                                                  \n"," tf.__operators__.add_3 (TF  (None, 512, 128)             0         ['tf.__operators__.add_2[0][0]\n"," OpLambda)                                                          ',                            \n","                                                                     'tf.math.multiply_3[0][0]']  \n","                                                                                                  \n","==================================================================================================\n","Total params: 4372608 (16.68 MB)\n","Trainable params: 4372608 (16.68 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n"]}],"source":["input_size = 512\n","embed_size = 128\n","vocab_size = len(tokenizer.get_vocab().keys()) + 1\n","\n","# Encoder\n","inputs_enc = Input(shape=(input_size, ), dtype=tf.int32)\n","emb_layer = Embedding(vocab_size, embed_size)\n","pos_layer = keras_nlp.layers.PositionEmbedding(input_size)\n","\n","x = LayerNormalization()(emb_layer(inputs_enc))\n","pos = pos_layer(x)\n","\n","b = 4\n","for _ in range(b):\n","    x += b**-0.5 * LayerNormalization()(Attention()(x, pos))\n","\n","encoder = keras.Model(inputs=inputs_enc, outputs=x)\n","\n","# Decoder\n","inputs = Input(shape=(input_size, ), dtype=tf.int32)\n","x = encoder(inputs)\n","lm_head = Lambda(lambda x: tf.nn.softmax(tf.matmul(x, emb_layer.embeddings, transpose_b=True), axis=-1))\n","\n","b = 4\n","for _ in range(b):\n","    x1 = Dense(embed_size, activation=\"gelu\")(x)\n","    x1 = Dense(embed_size, activation=\"gelu\")(x1)\n","    x += b**-0.5 * LayerNormalization()(x1)\n","\n","x = lm_head(x)\n","\n","model = keras.Model(inputs=inputs, outputs=x)\n","model.compile(\n","    loss=keras.losses.SparseCategoricalCrossentropy(ignore_class=tokenizer.pad_token_id),\n","    optimizer=keras.optimizers.AdamW(learning_rate=0.001),\n","    metrics=[masked_accuracy, keras_nlp.metrics.Perplexity(mask_token_id=tokenizer.pad_token_id)],\n",")\n","\n","encoder.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"22dadeab-5d9b-4ec2-8a52-8ce784ddbc06","_uuid":"ebdd11a0-e5ea-4948-8a9f-64895de659e4","collapsed":false,"execution":{"iopub.execute_input":"2024-07-20T00:17:39.502455Z","iopub.status.busy":"2024-07-20T00:17:39.502057Z"},"jupyter":{"outputs_hidden":false},"scrolled":true,"trusted":true},"outputs":[],"source":["for i in range(70):\n","    x, w = get_train_batch(4096*8, input_size+1)\n","    if i > 10 and i < 25:\n","        w = tf.where(w < 0.9, 0.05, 1.0)\n","    model.fit(x=x[:, :-1], y=x[:, 1:], shuffle=True, epochs=1, batch_size=16, sample_weight=w[:, 1:])\n","    model.save(\"model_slm.hdf5\")"]},{"cell_type":"code","execution_count":14,"metadata":{"_cell_guid":"62ff191d-4ec6-4fa7-a9b8-dad29e9c120a","_uuid":"b3149792-0bcb-455e-902d-0a2090c602cf","collapsed":false,"execution":{"iopub.execute_input":"2024-07-29T13:04:35.805547Z","iopub.status.busy":"2024-07-29T13:04:35.805183Z","iopub.status.idle":"2024-07-29T13:04:37.638388Z","shell.execute_reply":"2024-07-29T13:04:37.637284Z","shell.execute_reply.started":"2024-07-29T13:04:35.805516Z"},"jupyter":{"outputs_hidden":false},"scrolled":true,"trusted":true},"outputs":[],"source":["model = keras.models.load_model(\n","    \"model_slm.hdf5\",\n","    custom_objects={\n","        \"Attention\"       : Attention,\n","        \"masked_accuracy\" : masked_accuracy,\n","    },\n","    safe_mode=False,\n",")\n","# Extract Encoder\n","encoder = model.layers[1]"]},{"cell_type":"code","execution_count":11,"metadata":{"_cell_guid":"855c5834-248c-4415-95fb-b2d8c2cc7eca","_uuid":"591038fb-c92f-41c3-b849-7a02cb96c171","collapsed":false,"execution":{"iopub.execute_input":"2024-07-29T13:08:30.049561Z","iopub.status.busy":"2024-07-29T13:08:30.049014Z","iopub.status.idle":"2024-07-29T13:08:30.252628Z","shell.execute_reply":"2024-07-29T13:08:30.251460Z","shell.execute_reply.started":"2024-07-29T13:08:30.049515Z"},"jupyter":{"outputs_hidden":false},"scrolled":true,"trusted":true},"outputs":[],"source":["def vectorize_texts(all_texts):\n","    batch_size = 128\n","    vects = []\n","    for i in range(len(all_texts) // batch_size + 1):\n","        texts = all_texts[i*batch_size:i*batch_size+batch_size]\n","        toks = [text + ([tokenizer.pad_token_id] * (input_size - len(text))) for text in texts]\n","        toks = tf.constant(toks, shape=(len(toks), input_size))\n","        vect = encoder(toks)\n","        for v, t in zip(vect, texts):\n","            vects.append(v[:len(t), :])\n","    return tf.concat(vects, axis=0)\n","\n","vectorize_texts([tokenizer.encode(\"Hello. How have you been?\"), tokenizer.encode(\"hello\")])"]},{"cell_type":"code","execution_count":19,"metadata":{"_cell_guid":"9cd9518a-f88a-46d0-88ef-31fd6fc81de6","_uuid":"0b763816-dd9c-4b22-894e-75d0ae5e1cdd","collapsed":false,"execution":{"iopub.execute_input":"2024-07-29T13:08:31.224258Z","iopub.status.busy":"2024-07-29T13:08:31.222829Z","iopub.status.idle":"2024-07-29T14:26:11.184387Z","shell.execute_reply":"2024-07-29T14:26:11.183110Z","shell.execute_reply.started":"2024-07-29T13:08:31.224184Z"},"jupyter":{"outputs_hidden":false},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["127\n","255\n","383\n","511\n","639\n","767\n","895\n","1023\n","1151\n","1279\n","1407\n","1535\n","1663\n","1791\n","1919\n","2047\n","2175\n","2303\n","2431\n","2559\n","2687\n","2815\n","2943\n","3071\n","3199\n","3327\n","3455\n","3583\n","3711\n","3839\n","3967\n","4095\n","4223\n","4351\n","4479\n","4607\n","4735\n","4863\n","4991\n","5119\n","5247\n","5375\n","5503\n","5631\n","5759\n","5887\n","6015\n","6143\n","6271\n","6399\n","6527\n","6655\n","6783\n","6911\n","7039\n","7167\n","7295\n","7423\n","7551\n","7679\n","7807\n","7935\n","8063\n","8191\n","8319\n","8447\n","8575\n","8703\n","8831\n","8959\n","9087\n","9215\n","9343\n","9471\n","9599\n","9727\n","9855\n","9983\n","10111\n","10239\n","10367\n","10495\n","10623\n","10751\n","10879\n","11007\n","11135\n","11263\n","11391\n","11519\n","11647\n","11775\n","11903\n","12031\n","12159\n","12287\n","12415\n","12543\n","12671\n","12799\n","12927\n","13055\n","13183\n","13311\n","13439\n","13567\n","13695\n","13823\n","13951\n","14079\n","14207\n","14335\n","14463\n","14591\n","14719\n","14847\n","14975\n","15103\n","15231\n","15359\n","15487\n","15615\n","15743\n","15871\n","15999\n","16127\n","16255\n","16383\n","16511\n","16639\n","16767\n","16895\n","17023\n","17151\n","17279\n","17407\n","17535\n","17663\n","17791\n","17919\n","18047\n","18175\n","18303\n","18431\n","18559\n","18687\n","18815\n","18943\n","19071\n","19199\n","19327\n","19455\n","19583\n","19711\n","19839\n","19967\n","20095\n","20223\n","20351\n","20479\n","20607\n","20735\n","20863\n","20991\n","21119\n","21247\n","21375\n","21503\n","21631\n","21759\n","21887\n","22015\n","22143\n","22271\n","22399\n","22527\n","22655\n","22783\n","22911\n","23039\n","23167\n","23295\n","23423\n","23551\n","23679\n","23807\n","23935\n","24063\n","24191\n","24319\n","24447\n","24575\n","24703\n","24831\n","24959\n","25087\n","25215\n","25343\n","25471\n","25599\n","25727\n","25855\n","25983\n","26111\n","26239\n","26367\n","26495\n","26623\n","26751\n","26879\n","27007\n","27135\n","27263\n","27391\n","27519\n","27647\n","27775\n","27903\n","28031\n","28159\n","28287\n","28415\n","28543\n","28671\n","28799\n","28927\n","29055\n","29183\n","29311\n","29439\n","29567\n","29695\n","29823\n","29951\n","30079\n","30207\n","30335\n","30463\n","30591\n","30719\n","30847\n","30975\n","31103\n","31231\n","31359\n","31487\n","31615\n","31743\n","31871\n","31999\n","32127\n","32255\n","32383\n","32511\n","32639\n","32767\n","32895\n","33023\n","33151\n","33279\n","33407\n","33535\n","33663\n","33791\n","33919\n","34047\n","34175\n","34303\n","34431\n","34559\n","34687\n","34815\n","34943\n","35071\n","35199\n","35327\n","35455\n","35583\n","35711\n","35839\n","35967\n","36095\n","36223\n","36351\n","36479\n","36607\n","36735\n","36863\n","36991\n","37119\n","37247\n","37375\n","37503\n","37631\n","37759\n","37887\n","38015\n","38143\n","38271\n","38399\n","38527\n","38655\n","38783\n","38911\n","39039\n","39167\n","39295\n","39423\n","39551\n","39679\n","39807\n","39935\n","40063\n","40191\n","40319\n","40447\n","40575\n","40703\n","40831\n","40959\n","41087\n","41215\n","41343\n","41471\n","41599\n","41727\n","41855\n","41983\n","42111\n","42239\n","42367\n","42495\n","42623\n","42751\n","42879\n","43007\n","43135\n","43263\n","43391\n","43519\n","43647\n","43775\n","43903\n","44031\n","44159\n","44287\n","44415\n","44543\n","44671\n","44799\n","44927\n","45055\n","45183\n","45311\n","45439\n","45567\n","45695\n","45823\n","45951\n","46079\n","46207\n","46335\n","46463\n","46591\n","46719\n","46847\n","46975\n","47103\n","47231\n","47359\n","47487\n","47615\n","47743\n","47871\n","47999\n","48127\n","48255\n","48383\n","48511\n","48639\n","48767\n","48895\n","49023\n","49151\n","49279\n","49407\n","49535\n","49663\n","49791\n","49919\n","50047\n","50175\n","50303\n","50431\n","50559\n","50687\n","50815\n","50943\n","51071\n","51199\n","51327\n","51455\n","51583\n","51711\n","51839\n","51967\n","52095\n","52223\n","52351\n","52479\n","52607\n","52735\n","52863\n","52991\n","53119\n","53247\n","53375\n","53503\n","53631\n","53759\n","53887\n","54015\n","54143\n","54271\n","54399\n","54527\n","54655\n","54783\n","54911\n","55039\n","55167\n","55295\n","55423\n","55551\n","55679\n","55807\n","55935\n","56063\n","56191\n","56319\n","56447\n","56575\n","56703\n","56831\n","56959\n","57087\n","57215\n","57343\n","57471\n","57599\n","57727\n","57855\n","57983\n","58111\n","58239\n","58367\n","58495\n","58623\n","58751\n","58879\n","59007\n","59135\n","59263\n","59391\n","59519\n","59647\n","59775\n","59903\n","60031\n","60159\n","60287\n","60415\n","60543\n","60671\n","60799\n","60927\n","61055\n","61183\n","61311\n","61439\n","61567\n","61695\n","61823\n","61951\n","62079\n","62207\n","62335\n","62463\n","62591\n","62719\n","62847\n","62975\n","63103\n","63231\n","63359\n","63487\n","63615\n","63743\n","63871\n","63999\n","64127\n","64255\n","64383\n","64511\n","64639\n","64767\n","64895\n","65023\n","65151\n","65279\n","65407\n","65535\n","65663\n","65791\n","65919\n","66047\n","66175\n","66303\n","66431\n","66559\n","66687\n","66815\n","66943\n","67071\n","67199\n","67327\n","67455\n","67583\n","67711\n","67839\n","67967\n","68095\n","68223\n","68351\n","68479\n","68607\n","68735\n","68863\n","68991\n","69119\n","69247\n","69375\n","69503\n","69631\n","69759\n","69887\n"]}],"source":["all_toks = []\n","prompt_embeds = []\n","\n","batch_size = 128\n","batch = []\n","cur_batch_size = 0\n","\n","for j, text in enumerate(data):\n","    text_size = min(len(text), input_size+1)\n","    all_toks += text[1:text_size]\n","    trail = text[:text_size-1]\n","    \n","    batch.append(trail)\n","    cur_batch_size += 1\n","    \n","    if cur_batch_size >= batch_size:\n","        prompt_embeds.append(vectorize_texts(batch))\n","        cur_batch_size = 0\n","        batch = []\n","        print(j)"]},{"cell_type":"code","execution_count":20,"metadata":{"_cell_guid":"64ddca83-30fa-4c91-a167-f309f2bef98a","_uuid":"9c071e43-a9eb-407d-a55b-4d9ac999e24d","collapsed":false,"execution":{"iopub.execute_input":"2024-07-29T14:26:11.187336Z","iopub.status.busy":"2024-07-29T14:26:11.186848Z","iopub.status.idle":"2024-07-29T14:26:17.035055Z","shell.execute_reply":"2024-07-29T14:26:17.033947Z","shell.execute_reply.started":"2024-07-29T14:26:11.187291Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["prompt_embeds = np.vstack(prompt_embeds).reshape((sum([len(v) for v in prompt_embeds]), embed_size))"]},{"cell_type":"code","execution_count":21,"metadata":{"_cell_guid":"a550bd69-2793-41a9-acf3-33b11130c37c","_uuid":"37021c6f-4309-4844-b750-e47c8368a24b","collapsed":false,"execution":{"iopub.execute_input":"2024-07-29T14:26:17.037085Z","iopub.status.busy":"2024-07-29T14:26:17.036690Z","iopub.status.idle":"2024-07-29T14:26:26.195495Z","shell.execute_reply":"2024-07-29T14:26:26.194217Z","shell.execute_reply.started":"2024-07-29T14:26:17.037053Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["import faiss\n","\n","index = faiss.IndexFlat(embed_size) # IndexHNSWFlat(embed_size, 32)\n","#index.train(prompt_embeds)\n","index.add(prompt_embeds)"]},{"cell_type":"code","execution_count":31,"metadata":{"_cell_guid":"f8ec8b2f-6361-4237-96b0-dc858c6301de","_uuid":"8dca5e69-f59e-46b0-ab4f-d1addfdbcfaa","collapsed":false,"execution":{"iopub.execute_input":"2024-07-29T14:49:07.783141Z","iopub.status.busy":"2024-07-29T14:49:07.782676Z","iopub.status.idle":"2024-07-29T14:51:25.787919Z","shell.execute_reply":"2024-07-29T14:51:25.786816Z","shell.execute_reply.started":"2024-07-29T14:49:07.783092Z"},"jupyter":{"outputs_hidden":false},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<s> Peter: Hello there!\n"," Mia: Hello there, do you follow baseball or MLB much?\n"," Peter: I do, although I haven't followed it much the past couple of years.\n"," Mia: I am the opposite, I have been following more closely these last few, especially this last season as my team almost made the world series.\n"," Peter: Their arrow means they cover everything from the old days.\n"," Mia: Nice, yeah I like it. Did you know that the cubs were the first team to win back to back World Series.\n"," Peter: Women were not allowed to wear a baseball uniform as to be able to play for their teams if the need arises\n"," Mia: Maybe it is his"]}],"source":["text1 = \"\"\"<s>Peter: Hello there!\\n\"\"\"\n","\n","text2 = \"\"\"<s>The dog is red and has five legs.\n","User: What color is the dog?\n","Assistant: red\n","User: How many legs does the dog have?\n","Assistant:\"\"\"\n","\n","k = 10\n","temp = 0.01\n","text = text1\n","size = 128\n","\n","enc_text = tokenizer.encode(text, add_special_tokens=False)\n","text     = tokenizer.decode(enc_text)\n","print(text, end=\"\")\n","\n","for t in range(size):    \n","    xq = vectorize_texts([enc_text])[-1]\n","    xq = np.array(xq).reshape((1, embed_size))\n","    D, I = index.search(xq, k)\n","    toks = [all_toks[i] for i in I[0]]\n","    dists_sft = tf.nn.softmax(-D[0] / temp, axis=-1)\n","    c = tf.random.categorical(tf.math.log([dists_sft]), num_samples=1)[0][0]\n","    tok = toks[c]\n","    \n","    enc_text += [tok]\n","    new_text = tokenizer.decode(enc_text)\n","    \n","    print(new_text[len(text):], end=\"\")\n","    \n","    text = new_text"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4261095,"sourceId":9059404,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":4}
